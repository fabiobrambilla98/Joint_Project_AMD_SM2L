{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from pyspark.sql.window import Window\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from graphframes import GraphFrame\n",
    "from pyspark.sql.types import *\n",
    "import multiprocessing\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/fabio/spark-3.2.4-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/10 20:31:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created\n",
      "Spark context created\n"
     ]
    }
   ],
   "source": [
    "spark_driver_memory = \"10g\"\n",
    "spark_executor_memory = \"6g\"\n",
    "spark_partial_results_folder = './partial_results'\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(\"spark.driver.memory\", spark_driver_memory) \\\n",
    "                    .config(\"spark.executor.memory\", spark_executor_memory) \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .config(\"spark.sql.autoBroadcastJoinThreshold\", 100 * 1024 * 1024)\\\n",
    "                    .config(\"spark.executor.cores\", \"10\") \\\n",
    "                    .getOrCreate()\n",
    "print(\"Spark session created\")\n",
    "sc = spark.sparkContext\n",
    "print(\"Spark context created\")\n",
    "\n",
    "\n",
    "if not os.path.exists(spark_partial_results_folder):\n",
    "    os.makedirs(spark_partial_results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('timestamp', StringType(), True),\n",
    "    StructField('from_bank', IntegerType(), True),\n",
    "    StructField('from_account', StringType(), True),\n",
    "    StructField('to_bank', IntegerType(), True),\n",
    "    StructField('to_account', StringType(), True),\n",
    "    StructField('amount_received', FloatType(), True),\n",
    "    StructField('receiving_currency', StringType(), True),\n",
    "    StructField('amount_paid', FloatType(), True),\n",
    "    StructField('payment_currency', StringType(), True),\n",
    "    StructField('payment_format', StringType(), True),\n",
    "    StructField('is_laundering', IntegerType(), True)])\n",
    "\n",
    "\n",
    "\n",
    "df = spark.read.csv(\"../dataset/HI-Small_Trans.csv\", header = False, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+-------------+-----+\n",
      "|       timestamp|from_bank|from_account|to_bank|to_account|amount_received|receiving_currency|amount_paid|payment_currency|payment_format|is_laundering|index|\n",
      "+----------------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+-------------+-----+\n",
      "|2022/09/01 00:20|       10|   8000EBD30|     10| 8000EBD30|        3697.34|         US Dollar|    3697.34|       US Dollar|  Reinvestment|            0|    1|\n",
      "|2022/09/01 00:20|     3208|   8000F4580|      1| 8000F5340|           0.01|         US Dollar|       0.01|       US Dollar|        Cheque|            0|    2|\n",
      "|2022/09/01 00:00|     3209|   8000F4670|   3209| 8000F4670|       14675.57|         US Dollar|   14675.57|       US Dollar|  Reinvestment|            0|    3|\n",
      "|2022/09/01 00:02|       12|   8000F5030|     12| 8000F5030|        2806.97|         US Dollar|    2806.97|       US Dollar|  Reinvestment|            0|    4|\n",
      "|2022/09/01 00:06|       10|   8000F5200|     10| 8000F5200|       36682.97|         US Dollar|   36682.97|       US Dollar|  Reinvestment|            0|    5|\n",
      "+----------------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"index\", monotonically_increasing_id())\n",
    "df = df.filter(col('index') > 0)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: timestamp, from_bank: int, from_account: string, to_bank: int, to_account: string, amount_received: float, receiving_currency: string, amount_paid: float, payment_currency: string, payment_format: string, is_laundering: int, index: bigint, year: int, month: int, day: int, hour: int, minute: int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumn(\"timestamp\", to_timestamp(\"timestamp\", \"yyyy/MM/dd HH:mm\"))\n",
    "\n",
    "# Split the timestamp column into separate components\n",
    "df = df.withColumn(\"year\", year(\"timestamp\"))\\\n",
    "                             .withColumn(\"month\", month(\"timestamp\"))\\\n",
    "                             .withColumn(\"day\", dayofmonth(\"timestamp\"))\\\n",
    "                             .withColumn(\"hour\", hour(\"timestamp\"))\\\n",
    "                             .withColumn(\"minute\", minute(\"timestamp\"))\n",
    "\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: timestamp, from_bank: int, from_account: string, to_bank: int, to_account: string, amount_received: float, receiving_currency: string, amount_paid: float, payment_currency: string, payment_format: string, is_laundering: int, index: bigint, year: int, month: int, day: int, hour: int, minute: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_df = df.orderBy(\"timestamp\")\n",
    "\n",
    "# Calculate row counts for splits\n",
    "total_rows = ordered_df.count()\n",
    "train_rows, validation_rows = int(total_rows * 0.6), int(total_rows * 0.2)\n",
    "test_rows = total_rows - train_rows - validation_rows\n",
    "\n",
    "# Add a dummy partition and assign row numbers based on ordered timestamps\n",
    "w = Window.partitionBy(lit(1)).orderBy(\"timestamp\")\n",
    "ordered_df = ordered_df.withColumn(\"row_number\", F.row_number().over(w))\n",
    "\n",
    "# Split and repartition the DataFrame into train, validation, and test sets based on row numbers\n",
    "train_df = ordered_df.filter(col(\"row_number\") <= train_rows).drop(\"row_number\", \"dummy_partition\").repartition(48)\n",
    "validation_df = ordered_df.filter(col(\"row_number\").between(train_rows + 1, train_rows + validation_rows)).drop(\"row_number\", \"dummy_partition\").repartition(48)\n",
    "test_df = ordered_df.filter(col(\"row_number\") > train_rows + validation_rows).drop(\"row_number\", \"dummy_partition\").repartition(48)\n",
    "\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trans_received(df, name):\n",
    "    \n",
    "    window = Window.partitionBy('to_account', 'day')\n",
    "    df1 = df.withColumn(\"transaction_received_per_day\", count('*').over(window))\n",
    "\n",
    "    window = Window.partitionBy('to_account', 'hour')\n",
    "    df1 = df1.withColumn(\"transaction_received_per_hour\", count('*').over(window))\n",
    "\n",
    "    window = Window.partitionBy('to_account', 'minute')\n",
    "    df1 = df1.withColumn(\"transaction_received_per_minute\", count('*').over(window))\n",
    "\n",
    "    #df.coalesce(48).write.parquet(os.path.join(spark_partial_results_folder, f'partial_{name}'), mode='overwrite')\n",
    "    df.unpersist()\n",
    "    return df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trans_send(df, name):\n",
    "    window = Window.partitionBy('from_account', 'day')\n",
    "    df1 = df.withColumn(\"transaction_send_per_day\", count('*').over(window))\n",
    "\n",
    "    window = Window.partitionBy('from_account', 'hour')\n",
    "    df1 = df1.withColumn(\"transaction_send_per_hour\", count('*').over(window))\n",
    "\n",
    "    window = Window.partitionBy('from_account', 'minute')\n",
    "    df1 = df1.withColumn(\"transaction_send_per_minute\", count('*').over(window))\n",
    "\n",
    "    df.unpersist()\n",
    "    return df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_since_last_trans(df, name):\n",
    "    windowSpec = Window.partitionBy(\"from_account\").orderBy(\"timestamp\")\n",
    "\n",
    "    # Usa la funzione 'lag' per ottenere il timestamp della transazione precedente\n",
    "    df1 = df.withColumn(\"prev_timestamp\", F.lag(df.timestamp).over(windowSpec))\n",
    "\n",
    "    # Calcola la differenza in minuti\n",
    "    df1 = df1.withColumn(\n",
    "        \"minutes_since_last_transaction\", \n",
    "        F.when(\n",
    "            F.isnull(df1.prev_timestamp), \n",
    "            -1\n",
    "        ).otherwise(\n",
    "            (F.unix_timestamp(df1.timestamp) - F.unix_timestamp(df1.prev_timestamp)) / 60\n",
    "        )\n",
    "    )\n",
    "    df1 = df1.drop('prev_timestamp')\n",
    "\n",
    "    df.unpersist()\n",
    "    return df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_amount_variation(df, name):\n",
    "    windowSpec = Window.partitionBy(\"from_account\").orderBy(\"timestamp\").rowsBetween(Window.unboundedPreceding, -1)\n",
    "\n",
    "    # Calcola l'importo medio delle transazioni precedenti\n",
    "    df1 = df.withColumn(\"avg_previous_amount\", F.coalesce(F.avg(\"amount_received\").over(windowSpec), F.lit(0)))\n",
    "\n",
    "    # Calcola la variazione dell'importo e utilizza 0 come valore di default se Ã¨ nulla\n",
    "    df1 = df1.withColumn(\"amount_variation\", F.coalesce(F.col(\"amount_received\") - F.col(\"avg_previous_amount\"), F.lit(0)))\n",
    "\n",
    "    \n",
    "    df.unpersist()\n",
    "    return df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trans_recurrence(df, name):\n",
    "    windowSpec = Window.partitionBy(\"from_account\", \"to_account\").orderBy(\"timestamp\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "    # Calcola la ricorrenza delle transazioni\n",
    "    df1 = df.withColumn(\"transaction_recurrence\", F.count(\"timestamp\").over(windowSpec))\n",
    "    \n",
    "    df.unpersist()\n",
    "    return df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unique_payment_format_last_day(df, name):\n",
    "    df1 = df.withColumn(\"timestamp_minutes\", F.unix_timestamp(\"timestamp\")/60)\n",
    "\n",
    "    # Crea una finestra partizionata per from_account e ordinata per timestamp_minutes\n",
    "    # e limita la finestra alle ultime 24 ore (1440 minuti)\n",
    "    windowSpec = Window.partitionBy(\"from_account\").orderBy(\"timestamp_minutes\")\\\n",
    "                .rangeBetween(-1440, Window.currentRow)\n",
    "\n",
    "    # Calcola i diversi payment_format utilizzati all'interno della finestra temporale\n",
    "    df1 = df1.withColumn(\"unique_payment_formats_last_day\", F.collect_set(\"payment_format\").over(windowSpec))\n",
    "\n",
    "    # Calcola il numero di metodi di pagamento unici utilizzati\n",
    "    df1 = df1.withColumn(\"num_unique_payment_formats_last_day\", F.size(\"unique_payment_formats_last_day\"))\n",
    "\n",
    "    df1 = df1.drop('unique_payment_formats_last_day')\n",
    "\n",
    "    \n",
    "    df.unpersist()\n",
    "    return df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unique_payment_currency_last_day(df, name):\n",
    "    windowSpec = Window.partitionBy(\"from_account\").orderBy(\"timestamp_minutes\")\\\n",
    "                .rangeBetween(-1440, Window.currentRow)\n",
    "\n",
    "    # Calcola i diversi payment_format utilizzati all'interno della finestra temporale\n",
    "    df1 = df.withColumn(\"unique_payment_currency_last_day\", F.collect_set(\"payment_currency\").over(windowSpec))\n",
    "\n",
    "    # Calcola il numero di metodi di pagamento unici utilizzati\n",
    "    df1 = df1.withColumn(\"num_unique_payment_currency_last_day\", F.size(\"unique_payment_currency_last_day\"))\n",
    "\n",
    "    df1 = df1.drop('timestamp_minutes').drop('unique_payment_formats_last_day').drop('unique_payment_currency_last_day')\n",
    "\n",
    "    \n",
    "    df.unpersist()\n",
    "    return df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    df1 = df.select(\"*\",\n",
    "                    F.count(\"from_account\").over(Window.partitionBy(\"from_account\", \"to_account\", F.date_format(\"timestamp\", \"yyyy-MM-dd\"))).alias(\"daily_trans_between_accounts\"),\n",
    "                    F.size(F.collect_set(\"to_account\").over(Window.partitionBy(\"from_account\", \"timestamp\"))).alias(\"num_dest_accounts\")\n",
    "                    )\n",
    "    \n",
    "    \n",
    "    df1.cache().count()\n",
    "    df.unpersist()\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding(df, name):\n",
    "\n",
    "    def label_columns(df, col1, col2 = None):\n",
    "        unique_columns = set(df.select(col1).orderBy(col1).distinct().rdd.flatMap(lambda x: x).collect())\n",
    "        if(col2 != None):\n",
    "            unique_columns.update(df.select(col2).distinct().rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "        # Create a dictionary to map accounts to indexes\n",
    "        column_to_index = {column: index for index, column in enumerate(unique_columns)}\n",
    "\n",
    "        # Create a UDF to map accounts to their indexes\n",
    "        from pyspark.sql.functions import udf\n",
    "        from pyspark.sql.types import IntegerType\n",
    "\n",
    "        column_to_index_udf = udf(lambda column: column_to_index[column], IntegerType())\n",
    "\n",
    "        # Add indexed columns to the DataFrame\n",
    "        df_to_return = df.withColumn(f\"{col1}_indexed\", column_to_index_udf(col(col1))).drop(col1).withColumnRenamed(f'{col1}_indexed', col1) \n",
    "        if(col2!=None):\n",
    "            df_to_return = df_to_return.withColumn(f\"{col2}_indexed\", column_to_index_udf(col(col2))).drop(col2).withColumnRenamed(f'{col2}_indexed', col2)\n",
    "        return df_to_return\n",
    "    \n",
    "    df1 = label_columns(df, 'from_account', 'to_account')\n",
    "    df1 = label_columns(df1, 'receiving_currency', 'payment_currency')\n",
    "    df1 = label_columns(df1, 'payment_format')\n",
    "\n",
    "    df1.coalesce(48).write.parquet(os.path.join(spark_partial_results_folder, f'partial_{name}'), mode='overwrite')\n",
    "    df.unpersist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_parquet(name):\n",
    "    df = pd.read_parquet(f'./partial_results/partial_{name}')\n",
    "    \n",
    "    df.drop('index', axis=1, inplace=True)\n",
    "\n",
    "    df['same_account'] = df['from_account'] == df['to_account']\n",
    "    df['same_bank'] = df['from_bank'] == df['to_bank']\n",
    "    df['same_currency'] = df['receiving_currency'] == df['payment_currency']\n",
    "    df['same_amount'] = df['amount_received'] == df['amount_paid']\n",
    "\n",
    "    df.drop('timestamp', axis=1, inplace=True)\n",
    "    df.drop('from_bank', axis=1, inplace=True)\n",
    "    df.drop('to_bank', axis=1, inplace=True)\n",
    "    df.drop('from_account', axis=1, inplace=True)\n",
    "    df.drop('to_account', axis=1, inplace=True)\n",
    "    df.drop('year', axis=1, inplace=True)\n",
    "    df.drop('month', axis=1, inplace=True)\n",
    "\n",
    "    column = df.pop(\"is_laundering\")\n",
    "    df['is_laundering'] = column\n",
    "\n",
    "    \n",
    "    columns_to_encode = ['receiving_currency', 'payment_currency']\n",
    "    combined_values = df[columns_to_encode].values.ravel()\n",
    "    unique_combined_values = pd.unique(combined_values)\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(unique_combined_values)\n",
    "    for column in columns_to_encode:\n",
    "        df[column] = encoder.transform(df[column])\n",
    "\n",
    "    encoder.fit(df['payment_format'])\n",
    "    df['payment_format'] = encoder.transform(df['payment_format'])\n",
    "\n",
    "    columns_to_convert = ['same_account', 'same_bank', 'same_currency', 'same_amount', 'is_laundering']\n",
    "    df[columns_to_convert] = df[columns_to_convert].astype(int)\n",
    "\n",
    "    df.drop('amount_received', axis=1, inplace=True)\n",
    "    df.drop('amount_paid', axis=1, inplace=True)\n",
    "\n",
    "    df.to_parquet(f'./preprocessed_data/{name}_small', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, name):\n",
    "    df = add_trans_received(df, name)\n",
    "    print(f'{name}: Added trans received')\n",
    "    df = add_trans_send(df, name)\n",
    "    print(f'{name}: Added trans send')\n",
    "    df = add_time_since_last_trans(df, name)\n",
    "    print(f'{name}: add_time_since_last_trans')\n",
    "    df = add_amount_variation(df, name)\n",
    "    print(f'{name}: add_amount_variation')\n",
    "    df = add_unique_payment_format_last_day(df, name)\n",
    "    print(f'{name}: add_unique_payment_format_last_day')\n",
    "    df = add_unique_payment_currency_last_day(df, name)\n",
    "    print(f'{name}: add_unique_payment_currency_last_day')\n",
    "    df = create_features(df)\n",
    "    print(f'{name}: add_features_day')\n",
    "    df = label_encoding(df, name)\n",
    "    print(f'{name}: label_encoding')\n",
    "    save_as_parquet(name)\n",
    "    print(f'{name}: saved - finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: Added trans received\n",
      "train: Added trans send\n",
      "train: add_time_since_last_trans\n",
      "train: add_amount_variation\n",
      "train: add_unique_payment_format_last_day\n",
      "train: add_unique_payment_currency_last_day\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/10 20:31:47 WARN CacheManager: Asked to cache already cached data.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: add_features_day\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: label_encoding\n",
      "train: saved - finish\n",
      "val: Added trans received\n",
      "val: Added trans send\n",
      "val: add_time_since_last_trans\n",
      "val: add_amount_variation\n",
      "val: add_unique_payment_format_last_day\n",
      "val: add_unique_payment_currency_last_day\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: add_features_day\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: label_encoding\n",
      "val: saved - finish\n",
      "test: Added trans received\n",
      "test: Added trans send\n",
      "test: add_time_since_last_trans\n",
      "test: add_amount_variation\n",
      "test: add_unique_payment_format_last_day\n",
      "test: add_unique_payment_currency_last_day\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: add_features_day\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: label_encoding\n",
      "test: saved - finish\n"
     ]
    }
   ],
   "source": [
    "preprocess_data(train_df, 'train')\n",
    "preprocess_data(validation_df, 'val')\n",
    "preprocess_data(test_df, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import tqdm\n",
    "from pyspark.sql.window import Window\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from graphframes import GraphFrame\n",
    "from pyspark.sql.types import *\n",
    "import multiprocessing\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_driver_memory = \"8g\"\n",
    "spark_executor_memory = \"4g\"\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(\"spark.driver.memory\", spark_driver_memory) \\\n",
    "                    .config(\"spark.executor.memory\", spark_executor_memory) \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .getOrCreate()\n",
    "print(\"Spark session created\")\n",
    "sc = spark.sparkContext\n",
    "print(\"Spark context created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('timestamp', StringType(), True),\n",
    "    StructField('from_bank', IntegerType(), True),\n",
    "    StructField('from_account', StringType(), True),\n",
    "    StructField('to_bank', IntegerType(), True),\n",
    "    StructField('to_account', StringType(), True),\n",
    "    StructField('amount_received', FloatType(), True),\n",
    "    StructField('receiving_currency', StringType(), True),\n",
    "    StructField('amount_paid', FloatType(), True),\n",
    "    StructField('payment_currency', StringType(), True),\n",
    "    StructField('payment_format', StringType(), True),\n",
    "    StructField('is_laundering', IntegerType(), True)])\n",
    "\n",
    "\n",
    "\n",
    "spark_df = spark.read.csv(\"../dataset/HI-Small_Trans.csv\", header = False, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn(\"index\", monotonically_increasing_id())\n",
    "spark_df = spark_df.filter(col('index') > 0)\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proportion Laundering and not Laundering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = spark_df.count()\n",
    "spark_df.select('is_laundering').groupBy('is_laundering').agg(count('*').alias('count')).withColumn(\"proportion\", col('count')/total_count).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display payment format in relation to laundering transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark_df.select('payment_format', 'is_laundering') \\\n",
    "    .groupBy('payment_format') \\\n",
    "    .agg(\n",
    "        sum(col('is_laundering').cast('int')).alias('1'),\n",
    "        sum((1 - col('is_laundering')).cast('int')).alias('0')\n",
    "    ).orderBy('1', ascending=False).show(truncate=False)\n",
    "\n",
    "# Calculate the number of corresponding values for each value of the \"Payment Format\" and \"Is Laundering\" columns\n",
    "grouped_df = spark_df.groupBy(\"payment_format\", \"is_laundering\").count()\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "count_values = grouped_df.toPandas()\n",
    "\n",
    "# Use the unstack() method\n",
    "count_values_payment = count_values.pivot(index='payment_format', columns='is_laundering', values='count')\n",
    "\n",
    "# Create a bar chart with a logarithmic scale\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "bar_width = 0.35\n",
    "bar_positions = range(len(count_values_payment.index))\n",
    "axs[0].bar(bar_positions, count_values_payment[0], bar_width, label='Is Laundering = 0')\n",
    "axs[0].bar([p + bar_width for p in bar_positions], count_values_payment[1], bar_width, label='Is Laundering = 1')\n",
    "axs[0].set_xticks(bar_positions)\n",
    "axs[0].set_xticklabels(count_values_payment.index, rotation='vertical') \n",
    "axs[0].set_xlabel('Payment Format')\n",
    "axs[0].set_ylabel('Number of corresponding values')\n",
    "axs[0].set_title('Bar chart in arithmetic scale')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].bar(bar_positions, count_values_payment[0], bar_width, label='Is Laundering = 0')\n",
    "axs[1].bar([p + bar_width for p in bar_positions], count_values_payment[1], bar_width, label='Is Laundering = 1')\n",
    "axs[1].set_xticks(bar_positions)\n",
    "axs[1].set_xticklabels(count_values_payment.index, rotation='vertical') \n",
    "axs[1].set_xlabel('Payment Format')\n",
    "axs[1].set_ylabel('Number of corresponding values')\n",
    "axs[1].set_title('Bar chart in logarithmic scale')\n",
    "axs[1].legend()\n",
    "axs[1].set_yscale('log')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display payment currency in relation to laundering transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.select('payment_currency', 'is_laundering') \\\n",
    "    .groupBy('payment_currency') \\\n",
    "    .agg(\n",
    "        sum(col('is_laundering').cast('int')).alias('1'),\n",
    "        sum((1 - col('is_laundering')).cast('int')).alias('0')\n",
    "    ).orderBy('1', ascending=False).show(truncate=False)\n",
    "\n",
    "grouped_df = spark_df.groupBy(\"payment_currency\", \"is_laundering\").count()\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "count_values = grouped_df.toPandas()\n",
    "\n",
    "# Use the unstack() method\n",
    "count_values_currency = count_values.pivot(index='payment_currency', columns='is_laundering', values='count')\n",
    "\n",
    "# Sort the values by Is Laundering = 1 in descending order\n",
    "count_values_currency = count_values_currency.sort_values(1, ascending=False)\n",
    "\n",
    "# Create a bar chart with a logarithmic scale\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "bar_width = 0.35\n",
    "bar_positions = range(len(count_values_currency.index))\n",
    "axs[0].bar(bar_positions, count_values_currency[0], bar_width, label='Is Laundering = 0')\n",
    "axs[0].bar([p + bar_width for p in bar_positions], count_values_currency[1], bar_width, label='Is Laundering = 1')\n",
    "axs[0].set_xticks(bar_positions)\n",
    "axs[0].set_xticklabels(count_values_currency.index, rotation='vertical') \n",
    "axs[0].set_xticklabels(count_values_currency.index)\n",
    "axs[0].set_xlabel('Payment Currency')\n",
    "axs[0].set_ylabel('Number of corresponding values')\n",
    "axs[0].set_title('Bar chart in arithmetic scale')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].bar(bar_positions, count_values_currency[0], bar_width, label='Is Laundering = 0')\n",
    "axs[1].bar([p + bar_width for p in bar_positions], count_values_currency[1], bar_width, label='Is Laundering = 1')\n",
    "axs[1].set_xticks(bar_positions)\n",
    "axs[1].set_xticklabels(count_values_currency.index, rotation='vertical') \n",
    "axs[1].set_xticklabels(count_values_currency.index)\n",
    "axs[1].set_xlabel('Payment Currency')\n",
    "axs[1].set_ylabel('Number of corresponding values')\n",
    "axs[1].set_title('Bar chart in logarithmic scale')\n",
    "axs[1].legend()\n",
    "axs[1].set_yscale('log')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display top 10 accounts for fraudolent transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.select(col('from_account').alias('account'), col('is_laundering'))\\\n",
    ".filter(col('is_laundering') == 1).groupBy('account')\\\n",
    ".agg(count('*').alias('count_laundering'))\\\n",
    ".orderBy('count_laundering', ascending=False)\\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display top 20 accounts for transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.select(col('from_account').alias('account'))\\\n",
    ".groupBy('account')\\\n",
    ".agg(count('*').alias('count_transactions'))\\\n",
    ".orderBy('count_transactions', ascending=False)\\\n",
    ".show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display relationhip between amount paid and laundering transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_stats = spark_df.groupBy('is_laundering').agg(\n",
    "    min(col('amount_paid')).alias('min'),\n",
    "    max(col('amount_paid')).alias('max'),\n",
    "    mean(col('amount_paid')).alias('mean')\n",
    ")\n",
    "\n",
    "# Mostra le statistiche\n",
    "grouped_stats.show()\n",
    "\n",
    "\n",
    "# Estrai il DataFrame Spark come Pandas DataFrame\n",
    "df_pd = spark_df.toPandas()\n",
    "\n",
    "# Applica la scala logaritmica e la formattazione numerica\n",
    "plt.yscale(\"log\")\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: format(x, ',.2f')))\n",
    "\n",
    "# Crea il grafico scatter\n",
    "plt.scatter(df_pd['is_laundering'], df_pd['amount_paid'], alpha=0.5)\n",
    "plt.title(\"Relationship between Is Laundering and Amount Paid\")\n",
    "plt.xlabel(\"Is Laundering\")\n",
    "plt.ylabel(\"Amount Paid\")\n",
    "plt.xticks([0, 1])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\", \"yyyy/MM/dd HH:mm\"))\n",
    "\n",
    "# Split the timestamp column into separate components\n",
    "spark_df = spark_df.withColumn(\"year\", year(\"timestamp\"))\\\n",
    "                             .withColumn(\"month\", month(\"timestamp\"))\\\n",
    "                             .withColumn(\"day\", dayofmonth(\"timestamp\"))\\\n",
    "                             .withColumn(\"hour\", hour(\"timestamp\"))\\\n",
    "                             .withColumn(\"minute\", minute(\"timestamp\"))\n",
    "spark_df.persist()\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laundering_for(col_name: str):\n",
    "    print(f\"Laundering for {col_name}\")\n",
    "    spark_df.select(col_name, 'is_laundering') \\\n",
    "    .groupBy(col_name) \\\n",
    "    .agg(\n",
    "        sum(col('is_laundering').cast('int')).alias('count(1)'),\n",
    "        sum((1 - col('is_laundering')).cast('int')).alias('count(0)'),\n",
    "    ).withColumn(\"ratio\", (col('count(1)')/col('count(0)')).cast('Decimal(20,6)')) \\\n",
    "  .orderBy(col('ratio').desc()) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laundering_for('year')\n",
    "laundering_for('month')\n",
    "laundering_for('day')\n",
    "laundering_for('hour')\n",
    "laundering_for('minute')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count the number of transaction an account receive in different period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy('to_account', 'day')\n",
    "spark_df = spark_df.withColumn(\"transaction_received_per_day\", count('*').over(window))\n",
    "\n",
    "window = Window.partitionBy('to_account', 'hour')\n",
    "spark_df = spark_df.withColumn(\"transaction_received_per_hour\", count('*').over(window))\n",
    "\n",
    "window = Window.partitionBy('to_account', 'minute')\n",
    "spark_df = spark_df.withColumn(\"transaction_received_per_minute\", count('*').over(window))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count the number of transaction an account send in different period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy('from_account', 'day')\n",
    "spark_df = spark_df.withColumn(\"transaction_send_per_day\", count('*').over(window))\n",
    "\n",
    "window = Window.partitionBy('from_account', 'hour')\n",
    "spark_df = spark_df.withColumn(\"transaction_send_per_hour\", count('*').over(window))\n",
    "\n",
    "window = Window.partitionBy('from_account', 'minute')\n",
    "spark_df = spark_df.withColumn(\"transaction_send_per_minute\", count('*').over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.cache()\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode = ['from_account', 'to_account', 'receiving_currency', 'payment_currency', 'payment_format']\n",
    "\n",
    "# Applica l'indicizzazione delle stringhe a ciascuna colonna\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_index\").fit(spark_df) for column in columns_to_encode]\n",
    "indexed_df = spark_df\n",
    "for indexer in indexers:\n",
    "    indexed_df = indexer.transform(indexed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns_to_encode:\n",
    "    indexed_df = indexed_df.drop(column).withColumnRenamed(column + \"_index\", column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_cast = ['from_account', 'to_account', 'receiving_currency', 'payment_currency', 'payment_format']\n",
    "\n",
    "for column in columns_to_cast:\n",
    "    indexed_df = indexed_df.withColumn(column, col(column).cast(\"integer\"))\n",
    "\n",
    "df = indexed_df\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More correlation among Accounts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find how many times an Account send laund money and not laund money to the same Account.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.select('from_account', 'to_account', 'is_laundering')\n",
    "\n",
    "# Raggruppa e conta le occorrenze uniche\n",
    "grouped_df = df_temp.groupBy('from_account', 'to_account').agg(collect_set('is_laundering').alias('unique_values'))\n",
    "\n",
    "# Filtra i risultati con più di una occorrenza\n",
    "filtered_df = grouped_df.filter(col('unique_values').getItem(0) != col('unique_values').getItem(1))\n",
    "\n",
    "# Calcola il numero di occorrenze filtrate per ogni 'from_account'\n",
    "result_df = filtered_df.groupBy('from_account').count().orderBy(col('count').desc())\n",
    "\n",
    "# Mostra i primi 10 risultati\n",
    "result_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save first part of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('df.parquet3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('df.parquet1').drop(\"__index_level_0__\")\n",
    "\n",
    "df = df.withColumn(\"timestamp\",\n",
    "    concat_ws(\"\",\n",
    "        year(\"timestamp\"),\n",
    "        lpad(month(\"timestamp\"), 2, \"0\"),\n",
    "        lpad(dayofmonth(\"timestamp\"), 2, \"0\"),\n",
    "        lpad(hour(\"timestamp\"), 2, \"0\"),\n",
    "        lpad(minute(\"timestamp\"), 2, \"0\")\n",
    "    )\n",
    ")\n",
    "df = df.withColumn(\"timestamp\", col(\"timestamp\").cast(LongType()))\n",
    "df.orderBy('index').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using graph frame to work with graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all transactions that are send from A to B with a certain value and from B to C with the same value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = {}\n",
    "values = list(df.select(\"payment_format\").distinct().collect())\n",
    "j = 0\n",
    "for i in range(len(values)):\n",
    "    for k in range(0, len(values)):\n",
    "        combinations[(i,k)] = j\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_money_send_to_send(df):\n",
    "\n",
    "    train_vertices = df.select(F.col(\"from_account\").alias(\"id\")).union(df.select(F.col(\"to_account\").alias(\"id\"))).distinct()\n",
    "    train_edges = df.select(F.col(\"from_account\").alias(\"src\"), F.col(\"to_account\").alias(\"dst\"), F.col(\"index\"), F.col(\"amount_paid\").alias(\"amount\"), F.col(\"timestamp\"), F.col(\"payment_format\"), F.col(\"is_laundering\"))\n",
    "    g = GraphFrame(train_vertices, train_edges)\n",
    "\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"index\", LongType(), False),\n",
    "        StructField(\"timestamp\", DoubleType(), False),\n",
    "        StructField(\"from\", IntegerType(), False),\n",
    "        StructField(\"to\", IntegerType(), False),\n",
    "        StructField(\"payment_format\", IntegerType(), False),\n",
    "        StructField(\"is_laundering\", IntegerType(), False),\n",
    "        StructField(\"payment_payment\", IntegerType(), False)\n",
    "    ])\n",
    "\n",
    "\n",
    "    motif = \"(a)-[c1]->(b); (b)-[c2]->(c)\"\n",
    "    filter_string = \"a != b and b != c and c1.amount == c2.amount and c1.timestamp < c2.timestamp\"\n",
    "    graph = g.find(motif).filter(filter_string).distinct()\n",
    "    graph.cache()\n",
    "    columns = ['c1', 'c2']\n",
    "    pattern = np.array(graph.select(*columns).collect()).squeeze()\n",
    "    total_rows = []\n",
    "\n",
    "    for row in pattern:\n",
    "        rows_to_append = []\n",
    "        payment_formats = []\n",
    "        if isinstance(row[1], np.ndarray):\n",
    "            for r in row:\n",
    "                #index | timestamp | from | to | payment_format | is_laundering \n",
    "                rows_to_append.append([int(r[2]), float(r[4]), int(r[0]), int(r[1]), int(r[5]), int(r[6])])\n",
    "                payment_formats.append(int(r[5]))\n",
    "        else:\n",
    "                rows_to_append.append([int(r[2]), float(r[4]), int(r[0]), int(r[1]), int(r[5]), int(r[6])])\n",
    "                payment_formats.append(int(r[5]))\n",
    "        \n",
    "        for r in rows_to_append:\n",
    "            r.append(combinations[(payment_formats[0], payment_formats[1])])\n",
    "            total_rows.append(r)\n",
    "\n",
    "    temp_df = spark.createDataFrame(total_rows, schema)\n",
    "\n",
    "    temp_df = temp_df.dropDuplicates(['index'])\n",
    "\n",
    "    joined_df = df.join(temp_df.select(\"index\", \"payment_payment\").withColumnRenamed(\"payment_payment\", \"payment_payment_B\"), on=\"index\", how=\"left\")\n",
    "\n",
    "    # Aggiungi la colonna \"payment_payment\" a dfA, usando il valore corrispondente da dfB se presente, altrimenti imposta -1\n",
    "    df = joined_df.withColumn(\"payment_payment\", F.when(F.col(\"payment_payment_B\").isNotNull(), F.col(\"payment_payment_B\")).otherwise(-1)).drop(\"payment_payment_B\")\n",
    "  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_money_send_to_send(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find circular patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cyclesr(df):\n",
    "    schema = StructType([\n",
    "        StructField(\"index\", LongType(), False),\n",
    "        StructField(\"timestamp\", DoubleType(), False),\n",
    "        StructField(\"from\", IntegerType(), False),\n",
    "        StructField(\"to\", IntegerType(), False),\n",
    "        StructField(\"payment_format\", IntegerType(), False),\n",
    "        StructField(\"is_laundering\", IntegerType(), False),\n",
    "        StructField(\"hop_2\", IntegerType(), False),\n",
    "        StructField(\"hop_3\", IntegerType(), False),\n",
    "        StructField(\"hop_4\", IntegerType(), False),\n",
    "        StructField(\"hop_5\", IntegerType(), False),\n",
    "        StructField(\"hop_6\", IntegerType(), False),\n",
    "        StructField(\"hop_7\", IntegerType(), False),\n",
    "        StructField(\"hop_8\", IntegerType(), False),\n",
    "        StructField(\"hop_9\", IntegerType(), False),\n",
    "        StructField(\"hop_10\", IntegerType(), False),\n",
    "        StructField(\"hop_11\", IntegerType(), False),\n",
    "        StructField(\"hop_12\", IntegerType(), False),\n",
    "        StructField(\"hop_13\", IntegerType(), False)\n",
    "    ])\n",
    "\n",
    "    all_df = []\n",
    "    filtered_spark = df.filter(F.col(\"payment_currency\") == F.col(\"receiving_currency\"))\n",
    "    filtered_spark.persist()\n",
    "    for j in range(1):\n",
    "        verteces = (\n",
    "            filtered_spark.filter(F.col(\"payment_format\") == j)\\\n",
    "            .select(F.col(\"from_account\").alias(\"id\"))\\\n",
    "            .union(\n",
    "                filtered_spark.filter(F.col(\"payment_format\") == j).select(F.col(\"to_account\").alias(\"id\"))\n",
    "            )\\\n",
    "            .distinct()\n",
    "        )\n",
    "        \n",
    "        edges = (\n",
    "            filtered_spark.filter(F.col(\"payment_format\") == j)\\\n",
    "            .select(\n",
    "                F.col(\"from_account\").alias(\"src\"),\n",
    "                F.col(\"to_account\").alias(\"dst\"),\n",
    "                F.col(\"index\"),\n",
    "                F.col(\"amount_paid\").alias(\"amount\"),\n",
    "                F.col(\"timestamp\"),\n",
    "                F.col(\"payment_format\"),\n",
    "                F.col(\"is_laundering\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        \n",
    "        g = GraphFrame(verteces, edges)\n",
    "        g = g.dropIsolatedVertices()\n",
    "        g.persist()\n",
    "        for hop in tqdm.tqdm(range(2,14)):\n",
    "            motif = \"\"\n",
    "\n",
    "            for i in range(hop):\n",
    "                motif += \"(n\" + str(i) + \")-[c\" + str(i+1) + \"]->(n\" + str((i+1) % hop) + \"); \"\n",
    "            motif = motif.strip(\"; \")\n",
    "\n",
    "            filter_string = \"\"\n",
    "            for i in range(hop):\n",
    "                for j in range(i, hop-1):\n",
    "                    filter_string += \"n{} != n{}\".format(i, j+1)\n",
    "                    if i+1 < hop-1:\n",
    "                        filter_string += \" and \"\n",
    "            filter_string += \" and \"\n",
    "            for j in range(1,hop):\n",
    "                    filter_string += \"c{}.timestamp < c{}.timestamp\".format(j, j+1)\n",
    "                    if(j+1 < hop):\n",
    "                        filter_string += \" and \"    \n",
    "            graph = g.find(motif).filter(filter_string)\n",
    "            select_col = []\n",
    "            for i in range(hop):\n",
    "                select_col.append(\"c{}\".format(i+1))\n",
    "            pattern = np.array(graph.select(*select_col).collect()).squeeze()\n",
    "            total_rows = []\n",
    "            for row in pattern:\n",
    "                if isinstance(row[1], np.ndarray):\n",
    "                    for r in row:\n",
    "                        #index | timestamp | from | to | payment_format | is_laundering | hop\n",
    "                        total_rows.append([int(r[2]), int(r[4]), int(r[0]), int(r[1]), int(r[5]), int(r[6]), hop])\n",
    "                else:\n",
    "                    total_rows.append([int(row[2]), int(row[4]), int(row[0]), int(row[1]), int(row[5]), int(row[6]), hop])\n",
    "\n",
    "            dataframe = pd.DataFrame(total_rows, columns=['index', 'timestamp', 'from', 'to', 'payment_format', 'is_laundering', 'hop'])\n",
    "\n",
    "            all_df.append(dataframe.drop_duplicates())\n",
    "\n",
    "    merged_df = pd.concat(all_df, ignore_index=True)\n",
    "    one_hot_encoded_df = pd.get_dummies(merged_df, columns=['hop'], prefix='hop')\n",
    "\n",
    "    # Manually add missing hop columns (hop_2 to hop_13) and fill with False\n",
    "    columns_to_add = [f\"hop_{i}\" for i in range(2, 14)]\n",
    "    for col in columns_to_add:\n",
    "        if col not in one_hot_encoded_df.columns:\n",
    "            one_hot_encoded_df[col] = False\n",
    "\n",
    "\n",
    "\n",
    "    grouped_df = one_hot_encoded_df.groupby('index').agg({\n",
    "        'timestamp': 'first',\n",
    "        'from': 'first',\n",
    "        'to': 'first',\n",
    "        'payment_format': 'first',\n",
    "        'is_laundering': 'first',\n",
    "        **{col: 'any' for col in columns_to_add}\n",
    "    }).reset_index()\n",
    "   \n",
    "    columns_to_encode = ['hop_2', 'hop_3', 'hop_4', 'hop_5', 'hop_6', 'hop_7', 'hop_8', 'hop_9', 'hop_10', 'hop_11', 'hop_12', 'hop_13']\n",
    "    grouped_df[columns_to_encode] = grouped_df[columns_to_encode].fillna(False, inplace=False).astype(int)\n",
    "    \n",
    "    temp_df = spark.createDataFrame(grouped_df, schema)\n",
    "\n",
    "    temp_df = temp_df.dropDuplicates(['index'])\n",
    "\n",
    "    # Step 1: Seleziona solo le colonne necessarie da temp_df e rinomina le colonne\n",
    "    temp_df_selected = temp_df.select(\n",
    "        \"index\",\n",
    "        \"hop_2\", \"hop_3\", \"hop_4\" , \"hop_5\", \"hop_6\", \"hop_7\", \"hop_8\", \"hop_9\", \"hop_10\", \"hop_11\", \"hop_12\", \"hop_13\"\n",
    "    ).withColumnRenamed(\"hop_2\", \"hop_2_B\").withColumnRenamed(\"hop_3\", \"hop_3_B\").withColumnRenamed(\"hop_4\", \"hop_4_B\").withColumnRenamed(\"hop_5\", \"hop_5_B\").withColumnRenamed(\"hop_6\", \"hop_6_B\").withColumnRenamed(\"hop_7\", \"hop_7_B\").withColumnRenamed(\"hop_8\", \"hop_8_B\").withColumnRenamed(\"hop_9\", \"hop_9_B\").withColumnRenamed(\"hop_10\", \"hop_10_B\").withColumnRenamed(\"hop_11\", \"hop_11_B\").withColumnRenamed(\"hop_12\", \"hop_12_B\").withColumnRenamed(\"hop_13\", \"hop_13_B\")\n",
    "\n",
    "    # Step 2: Esegui una left join tra df e temp_df_selected, usando l'indice come chiave di join\n",
    "    joined_df = df.join(temp_df_selected, on=\"index\", how=\"left\")\n",
    "\n",
    "    # Step 3: Usa la funzione when per assegnare il valore corrispondente da hop_i_B se presente, altrimenti imposta il valore a 0\n",
    "    for i in range(2, 14):\n",
    "        joined_df = joined_df.withColumn(\n",
    "            f\"hop_{i}\", \n",
    "            F.when(F.col(f\"hop_{i}_B\").isNotNull(), F.col(f\"hop_{i}_B\")).otherwise(0)\n",
    "        )\n",
    "\n",
    "    # Step 4: Rimuovi le colonne aggiunte da temp_df_selected\n",
    "    joined_df = joined_df.drop(*[f\"hop_{i}_B\" for i in range(2, 14)])\n",
    "\n",
    "    # Il risultato finale è il dataframe df con le colonne hop_2 a hop_13 aggiunte e i valori 0 dove necessario\n",
    "    df_result = joined_df\n",
    "\n",
    "    g.unpersist()\n",
    "    filtered_spark.unpersist()\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cycles(df):\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"index\", LongType(), False),\n",
    "        StructField(\"timestamp\", LongType(), False),\n",
    "        StructField(\"from\", IntegerType(), False),\n",
    "        StructField(\"to\", IntegerType(), False),\n",
    "        StructField(\"payment_format\", IntegerType(), False),\n",
    "        StructField(\"is_laundering\", IntegerType(), False),\n",
    "        StructField(\"hop_2\", IntegerType(), False),\n",
    "        StructField(\"hop_3\", IntegerType(), False),\n",
    "        StructField(\"hop_4\", IntegerType(), False),\n",
    "        StructField(\"hop_5\", IntegerType(), False),\n",
    "        StructField(\"hop_6\", IntegerType(), False),\n",
    "        StructField(\"hop_7\", IntegerType(), False),\n",
    "        StructField(\"hop_8\", IntegerType(), False),\n",
    "        StructField(\"hop_9\", IntegerType(), False),\n",
    "        StructField(\"hop_10\", IntegerType(), False),\n",
    "        StructField(\"hop_11\", IntegerType(), False),\n",
    "        StructField(\"hop_12\", IntegerType(), False),\n",
    "        StructField(\"hop_13\", IntegerType(), False),\n",
    "    ])\n",
    "\n",
    "\n",
    "    all_df = []\n",
    "    filtered_spark = df.filter(F.col(\"payment_currency\") == F.col(\"receiving_currency\"))\n",
    "    filtered_spark.cache()\n",
    "    #payment_formats = filtered_spark.select(\"payment_format\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    for j in range(1):\n",
    "        verteces = filtered_spark.filter(F.col(\"payment_format\") == j).select(F.col(\"from_account\").alias(\"id\")).union(filtered_spark.filter(F.col(\"payment_format\") == j).select(F.col(\"to_account\").alias(\"id\"))).distinct()\n",
    "        edges = filtered_spark.filter(F.col(\"payment_format\") == j).select(F.col(\"from_account\").alias(\"src\"), F.col(\"to_account\").alias(\"dst\"), F.col(\"index\"), F.col(\"amount_paid\").alias(\"amount\"), F.col(\"timestamp\"), F.col(\"payment_format\"), F.col(\"is_laundering\"))\n",
    "        g = GraphFrame(verteces, edges)\n",
    "        g = g.dropIsolatedVertices()\n",
    "        g.cache()\n",
    "        for hop in tqdm.tqdm(range(2,14)):\n",
    "            motif = \"\"\n",
    "\n",
    "            for i in range(hop):\n",
    "                motif += \"(n\" + str(i) + \")-[c\" + str(i+1) + \"]->(n\" + str((i+1) % hop) + \"); \"\n",
    "            motif = motif.strip(\"; \")\n",
    "\n",
    "            filter_string = \"\"\n",
    "            for i in range(hop):\n",
    "                for j in range(i, hop-1):\n",
    "                    filter_string += \"n{} != n{}\".format(i, j+1)\n",
    "                    if i+1 < hop-1:\n",
    "                        filter_string += \" and \"\n",
    "            filter_string += \" and \"\n",
    "            for j in range(1,hop):\n",
    "                    filter_string += \"c{}.timestamp < c{}.timestamp\".format(j, j+1)\n",
    "                    if(j+1 < hop):\n",
    "                        filter_string += \" and \"    \n",
    "            graph = g.find(motif).filter(filter_string)\n",
    "            select_col = []\n",
    "            for i in range(hop):\n",
    "                select_col.append(\"c{}\".format(i+1))\n",
    "            pattern = np.array(graph.select(*select_col).collect()).squeeze()\n",
    "            total_rows = []\n",
    "\n",
    "            for row in pattern:\n",
    "                if isinstance(row[1], np.ndarray):\n",
    "                    for r in row:\n",
    "                        #index | timestamp | from | to | payment_format | is_laundering | hop\n",
    "                        total_rows.append([int(r[2]), r[4], int(r[0]), int(r[1]), int(r[5]), int(r[6]), hop])\n",
    "                else:\n",
    "                    total_rows.append([int(row[2]), row[4], int(row[0]), int(row[1]), int(row[5]), int(row[6]), hop])\n",
    "\n",
    "            dataframe = pd.DataFrame(total_rows, columns=['index', 'timestamp', 'from', 'to', 'payment_format', 'is_laundering', 'hop'])\n",
    "\n",
    "            all_df.append(dataframe.drop_duplicates())\n",
    "\n",
    "    merged_df = pd.concat(all_df, ignore_index=True)\n",
    "    one_hot_encoded_df = pd.get_dummies(merged_df, columns=['hop'], prefix='hop')\n",
    "\n",
    "    # Manually add missing hop columns (hop_2 to hop_13) and fill with False\n",
    "    columns_to_add = [f\"hop_{i}\" for i in range(2, 14)]\n",
    "    for col in columns_to_add:\n",
    "        if col not in one_hot_encoded_df.columns:\n",
    "            one_hot_encoded_df[col] = False\n",
    "\n",
    "    grouped_df = one_hot_encoded_df.groupby('index').agg({\n",
    "        'timestamp': 'first',\n",
    "        'from': 'first',\n",
    "        'to': 'first',\n",
    "        'payment_format': 'first',\n",
    "        'is_laundering': 'first',\n",
    "        **{col: 'any' for col in columns_to_add}\n",
    "    }).reset_index()\n",
    "   \n",
    "    columns_to_encode = ['hop_2', 'hop_3', 'hop_4', 'hop_5', 'hop_6', 'hop_7', 'hop_8', 'hop_9', 'hop_10', 'hop_11', 'hop_12', 'hop_13']\n",
    "    grouped_df[columns_to_encode] = grouped_df[columns_to_encode].fillna(False, inplace=False).astype(int)\n",
    "    \n",
    "    temp_df = spark.createDataFrame(grouped_df, schema)\n",
    "\n",
    "    temp_df = temp_df.dropDuplicates(['index'])\n",
    "\n",
    "    # Step 1: Seleziona solo le colonne necessarie da temp_df e rinomina le colonne\n",
    "    temp_df_selected = temp_df.select(\n",
    "        \"index\",\n",
    "        \"hop_2\", \"hop_3\", \"hop_4\", \"hop_5\", \"hop_6\",\n",
    "        \"hop_7\", \"hop_8\", \"hop_9\", \"hop_10\", \"hop_11\",\n",
    "        \"hop_12\", \"hop_13\"\n",
    "    ).withColumnRenamed(\"hop_2\", \"hop_2_B\").withColumnRenamed(\"hop_3\", \"hop_3_B\").withColumnRenamed(\"hop_4\", \"hop_4_B\").withColumnRenamed(\"hop_5\", \"hop_5_B\").withColumnRenamed(\"hop_6\", \"hop_6_B\").withColumnRenamed(\"hop_7\", \"hop_7_B\").withColumnRenamed(\"hop_8\", \"hop_8_B\").withColumnRenamed(\"hop_9\", \"hop_9_B\").withColumnRenamed(\"hop_10\", \"hop_10_B\").withColumnRenamed(\"hop_11\", \"hop_11_B\").withColumnRenamed(\"hop_12\", \"hop_12_B\").withColumnRenamed(\"hop_13\", \"hop_13_B\")\n",
    "\n",
    "    # Step 2: Esegui una left join tra df e temp_df_selected, usando l'indice come chiave di join\n",
    "    joined_df = df.join(temp_df_selected, on=\"index\", how=\"left\")\n",
    "\n",
    "    # Step 3: Usa la funzione when per assegnare il valore corrispondente da hop_i_B se presente, altrimenti imposta il valore a 0\n",
    "    for i in range(2, 14):\n",
    "        joined_df = joined_df.withColumn(\n",
    "            f\"hop_{i}\", \n",
    "            F.when(F.col(f\"hop_{i}_B\").isNotNull(), F.col(f\"hop_{i}_B\")).otherwise(0)\n",
    "        )\n",
    "\n",
    "    # Step 4: Rimuovi le colonne aggiunte da temp_df_selected\n",
    "    joined_df = joined_df.drop(*[f\"hop_{i}_B\" for i in range(2, 14)])\n",
    "\n",
    "    # Il risultato finale è il dataframe df con le colonne hop_2 a hop_13 aggiunte e i valori 0 dove necessario\n",
    "    df_result = joined_df\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "find_cycles(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find fan in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fanin(g: GraphFrame):\n",
    "    motif = \"(a)-[c1]->(b); (c)-[c2]->(b)\"\n",
    "    filter_motif = \"(abs(c1.timestamp - c2.timestamp)) <= 40000 and c1.index != c2.index and c1.payment_currency == c2.payment_currency\"#and c1.payment_format == c2.payment_format\"\n",
    "  \n",
    "    pattern = g.find(motif).filter(filter_motif).select(\"c1\", \"c2\").distinct()\n",
    "    fan_in_trans = pattern.groupBy(F.col(\"c1\")).agg(F.count(\"*\").alias(\"fan_in_degree\")).select(F.col(\"c1\").alias(\"transaction\"), F.col(\"fan_in_degree\"))\n",
    "    #fan_in_trans.cache()\n",
    "    return fan_in_trans\n",
    "\n",
    "def add_fan_in(df):\n",
    "    filtered_spark = df.filter(F.col(\"payment_currency\") == F.col(\"receiving_currency\"))\n",
    "    filtered_spark = filtered_spark.coalesce(12)\n",
    "    filtered_spark.cache()\n",
    "    payment_formats = filtered_spark.select(\"payment_format\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    total_fan_in = None\n",
    "\n",
    "    for payment_format in payment_formats:\n",
    "        print(f\"Find fan in payment_format: {payment_format}\")\n",
    "        filtered_by_format = filtered_spark.filter(F.col(\"payment_format\") == payment_format)\n",
    "        verteces = (\n",
    "            filtered_by_format.select(F.col(\"from_account\").alias(\"id\"))\n",
    "            .union(df.select(F.col(\"to_account\").alias(\"id\")))\n",
    "            .distinct()\n",
    "        )\n",
    "        edges = (\n",
    "            filtered_by_format.select(\n",
    "                F.col(\"from_account\").alias(\"src\"),\n",
    "                F.col(\"to_account\").alias(\"dst\"),\n",
    "                F.col(\"index\"),\n",
    "                F.col(\"timestamp\"),\n",
    "                F.col(\"payment_currency\"),\n",
    "                F.col(\"payment_format\"),\n",
    "                F.col(\"is_laundering\")\n",
    "                \n",
    "            )\n",
    "        )\n",
    "        g = GraphFrame(verteces, edges)\n",
    "        \n",
    "        if total_fan_in is None:\n",
    "            find_fan_in = find_fanin(g)\n",
    "            total_fan_in = find_fan_in\n",
    "        else:\n",
    "            find_fan_in = find_fanin(g)\n",
    "            total_fan_in = total_fan_in.unionAll(find_fan_in)\n",
    "           \n",
    "    \n",
    "    \n",
    "    def extract_values(transaction):\n",
    "        src, dst, index, timestamp, payment_currency, payment_format, is_laundering = transaction\n",
    "        return (src, dst, index, timestamp,payment_currency,  payment_format, is_laundering)\n",
    "\n",
    "    # Definisci lo schema per il DataFrame Spark\n",
    "    schema_udf = StructType([\n",
    "        StructField(\"src\", IntegerType(), True),\n",
    "        StructField(\"dst\", IntegerType(), True),\n",
    "        StructField(\"index\", IntegerType(), True),\n",
    "        StructField(\"timestamp\", FloatType(), True),\n",
    "        StructField(\"payment_currency\", IntegerType(), True),\n",
    "        StructField(\"payment_format\", IntegerType(), True),\n",
    "        StructField(\"is_laundering\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    # Applica la funzione UDF per estrarre i valori dalla colonna \"transaction\" e crea un nuovo DataFrame\n",
    "    extract_udf = F.udf(extract_values, schema_udf)\n",
    "    new_spark_df = total_fan_in.withColumn(\"extracted\", extract_udf(\"transaction\"))\n",
    "\n",
    "    # Seleziona le colonne necessarie e converte il DataFrame Spark in un DataFrame Pandas\n",
    "    temp_df = new_spark_df.select(\"extracted.*\", \"fan_in_degree\")\n",
    "\n",
    "    joined_df = df.join(temp_df.select(\"index\", \"fan_in_degree\").withColumnRenamed(\"fan_in_degree\", \"fan_in_degree_B\"), on=\"index\", how=\"left\")\n",
    "\n",
    "    # Aggiungi la colonna \"payment_payment\" a dfA, usando il valore corrispondente da dfB se presente, altrimenti imposta -1\n",
    "    df = joined_df.withColumn(\"fan_in_degree\", F.when(F.col(\"fan_in_degree_B\").isNotNull(), F.col(\"fan_in_degree_B\")).otherwise(0)).drop(\"fan_in_degree_B\")\n",
    "\n",
    "\n",
    "       \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_fan_in(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find fan out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "def find_fanout(g: GraphFrame, motif: str, filter_motif: str):\n",
    "    pattern = g.find(motif).filter(filter_motif).select(\"c1\", \"c2\").distinct()\n",
    "    fan_out_trans = pattern.groupBy(F.col(\"c1\")).agg(F.count(\"*\").alias(\"fan_out_degree\")).select(F.col(\"c1\").alias(\"transaction\"), F.col(\"fan_out_degree\"))\n",
    "    fan_out_trans.persist(StorageLevel.MEMORY_ONLY)  # Cache the result\n",
    "    return fan_out_trans\n",
    "\n",
    "def add_fan_out(df, num_partitions, total_fan_out=None):\n",
    "    filtered_spark = df.filter(F.col(\"payment_currency\") == F.col(\"receiving_currency\")).repartition(num_partitions)\n",
    "    filtered_spark.persist(StorageLevel.MEMORY_ONLY)  # Cache the filtered DataFrame\n",
    "\n",
    "    # Define motif and filter_motif for find_fanout function\n",
    "    motif = \"(a)-[c1]->(b); (a)-[c2]->(c)\"\n",
    "    filter_motif = \"(abs(c1.timestamp - c2.timestamp)) <= 40000 and c1.index != c2.index\"\n",
    "\n",
    "    if total_fan_out is None:\n",
    "        distinct_from_accounts = filtered_spark.select(\"from_account\").distinct().withColumnRenamed(\"from_account\", \"id\")\n",
    "        g = GraphFrame(distinct_from_accounts, \n",
    "                       filtered_spark.select(F.col(\"from_account\").alias(\"src\"), \n",
    "                                            F.col(\"to_account\").alias(\"dst\"), \n",
    "                                            \"index\", \"timestamp\", \"payment_format\", \"is_laundering\"))\n",
    "        total_fan_out = find_fanout(g, motif, filter_motif)\n",
    "    else:\n",
    "        distinct_from_accounts = filtered_spark.select(\"from_account\").distinct().withColumnRenamed(\"from_account\", \"id\")\n",
    "        g = GraphFrame(distinct_from_accounts, \n",
    "                       filtered_spark.select(F.col(\"from_account\").alias(\"src\"), \n",
    "                                            F.col(\"to_account\").alias(\"dst\"), \n",
    "                                            \"index\", \"timestamp\", \"payment_format\", \"is_laundering\"))\n",
    "        find_fan_out = find_fanout(g, motif, filter_motif)\n",
    "        total_fan_out = total_fan_out.unionAll(find_fan_out)\n",
    "        find_fan_out.unpersist()\n",
    "\n",
    "    def extract_values(transaction):\n",
    "        src, dst, index, timestamp, payment_format, is_laundering = transaction\n",
    "        return (src, dst, index, timestamp, payment_format, is_laundering)\n",
    "\n",
    "    schema_udf = StructType([\n",
    "        StructField(\"src\", IntegerType(), True),\n",
    "        StructField(\"dst\", IntegerType(), True),\n",
    "        StructField(\"index\", IntegerType(), True),\n",
    "        StructField(\"timestamp\", FloatType(), True),\n",
    "        StructField(\"payment_format\", IntegerType(), True),\n",
    "        StructField(\"is_laundering\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    extract_udf = F.udf(extract_values, schema_udf)\n",
    "    new_spark_df = total_fan_out.withColumn(\"extracted\", extract_udf(\"transaction\"))\n",
    "\n",
    "    temp_df = new_spark_df.select(\"extracted.*\", \"fan_out_degree\")\n",
    "\n",
    "    joined_df = df.join(temp_df.select(\"index\", \"fan_out_degree\").withColumnRenamed(\"fan_out_degree\", \"fan_out_degree_B\"), on=\"index\", how=\"left\")\n",
    "\n",
    "    df = joined_df.withColumn(\"fan_out_degree\", F.when(F.col(\"fan_out_degree_B\").isNotNull(), F.col(\"fan_out_degree_B\")).otherwise(0)).drop(\"fan_out_degree_B\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_fan_out(df, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display correlation matrix for fraudolent transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleziona le colonne numeriche su cui calcolare la correlazione\n",
    "numeric_columns = ['from_bank', 'to_bank', 'from_account', 'to_account','receiving_currency','payment_currency','payment_format', 'amount_received', 'amount_paid',  'day', 'hour', 'minute', 'transaction_received_per_day',  'transaction_received_per_hour',  'transaction_received_per_minute', 'transaction_send_per_day',  'transaction_send_per_hour',  'transaction_send_per_minute','payment_payment',  'fan_in_degree','hop_2', 'hop_3', 'hop_4', 'hop_5', 'hop_6', 'hop_7', 'hop_8', 'hop_9', 'hop_10', 'hop_11', 'hop_12', 'hop_13','is_laundering']\n",
    "\n",
    "# Crea un VectorAssembler per creare una singola colonna \"features\"\n",
    "assembler = VectorAssembler(inputCols=numeric_columns, outputCol=\"features\")\n",
    "assembled_df = assembler.transform(df).select(\"features\")\n",
    "\n",
    "\n",
    "# Calcola la matrice di correlazione per entrambi i casi\n",
    "laundering_corr_matrix = df.select(numeric_columns).toPandas().corr()\n",
    "\n",
    "# create subplots\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "\n",
    "# plot the first correlation matrix heatmap\n",
    "sns.heatmap(laundering_corr_matrix, cmap='coolwarm', annot=False, ax=ax)\n",
    "ax.set_title('Correlation matrix')\n",
    "\n",
    "# display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Laundering:\")\n",
    "count_amount, count_currency = len(df[(df['Amount Received'] == df['Amount Paid']) & (df['Is Laundering'] == 1)]), len(df[(df['Receiving Currency'] == df['Payment Currency']) & (df['Is Laundering'] == 1)])\n",
    "print(f\"    Same amount: {count_amount}\")\n",
    "print(f\"    Same currency: {count_currency}\")\n",
    "print(f\"    Difference: {np.abs(count_amount - count_currency)}\\n\")\n",
    "\n",
    "count_amount, count_currency = len(df[(df['Amount Received'] != df['Amount Paid']) & (df['Is Laundering'] == 1)]), len(df[(df['Receiving Currency'] != df['Payment Currency']) & (df['Is Laundering'] == 1)])\n",
    "print(f\"    Different amount: {count_amount}\")\n",
    "print(f\"    Different currency: {count_currency}\")\n",
    "print(f\"    Difference: {np.abs(count_amount - count_currency)}\\n\")\n",
    "\n",
    "print(\"Not Laundering:\")\n",
    "count_amount, count_currency = len(df[(df['Amount Received'] == df['Amount Paid']) & (df['Is Laundering'] == 0)]), len(df[(df['Receiving Currency'] == df['Payment Currency']) & (df['Is Laundering'] == 0)])\n",
    "print(f\"    Same amount: {count_amount}\")\n",
    "print(f\"    Same currency: {count_currency}\")\n",
    "print(f\"    Difference: {np.abs(count_amount - count_currency)}\\n\")\n",
    "\n",
    "count_amount, count_currency = len(df[(df['Amount Received'] != df['Amount Paid']) & (df['Is Laundering'] == 0)]), len(df[(df['Receiving Currency'] != df['Payment Currency']) & (df['Is Laundering'] == 0)])\n",
    "print(f\"    Different amount: {count_amount}\")\n",
    "print(f\"    Different currency: {count_currency}\")\n",
    "print(f\"    Difference: {np.abs(count_amount - count_currency)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/fabio/jars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('timestamp', FloatType(), True),\n",
    "    StructField('from_bank', IntegerType(), True),\n",
    "    StructField('from_account', IntegerType(), True),\n",
    "    StructField('to_bank', IntegerType(), True),\n",
    "    StructField('to_account', IntegerType(), True),\n",
    "    StructField('amount_received', FloatType(), True),\n",
    "    StructField('receiving_currency', IntegerType(), True),\n",
    "    StructField('amount_paid', FloatType(), True),\n",
    "    StructField('payment_currency', IntegerType(), True),\n",
    "    StructField('payment_format', IntegerType(), True),\n",
    "    StructField('is_laundering', IntegerType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drename(columns={'Timestamp': 'timestamp', 'From Bank': 'from_bank', 'Account': 'from_account',\n",
    "                           'To Bank': 'to_bank', 'Account.1': 'to_account', 'Amount Received': 'amount_received',\n",
    "                             'Receiving Currency': 'receiving_currency', 'Amount Paid': 'amount_paid', 'Payment Currency': 'payment_currency',\n",
    "                               'Payment Format': 'payment_format', 'Is Laundering': 'is_laundering'})\n",
    "dto_parquet('dparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verteces = spark_df.select(col(\"from_account\").alias(\"id\")).union(spark_df.select(col(\"to_account\").alias(\"id\"))).distinct()\n",
    "edges = spark_df.select(col(\"from_account\").alias(\"src\"), col(\"to_account\").alias(\"dst\"), col(\"index\"), col(\"amount_paid\").alias(\"amount\"), col(\"timestamp\"), col(\"payment_format\"), col(\"is_laundering\"))\n",
    "g = GraphFrame(verteces, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = g.find(\"(a)-[c1]->(b); (b)-[c2]->(c) \").filter(\"\"\"\n",
    "                                              a != b and\n",
    "                                              b != c and\n",
    "\n",
    "                                              c1.amount == c2.amount and\n",
    "                                              c1.timestamp < c2.timestamp\n",
    "                                            \"\"\")\n",
    "pattern.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_features =  np.array(pattern.select('c1','c2').collect(), dtype=int).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "dictionary = defaultdict(list)\n",
    "for array in array_features:\n",
    "    dictionary[(array[0][5], array[1][5])].append((array[0][6], array[1][6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(list(count_values_payment.reset_index()['Payment Format']))\n",
    "c = Counter()\n",
    "l = []\n",
    "matrix = [[0 for _ in range(6)] for _ in range(7)]\n",
    "data = {}\n",
    "for key, items in dictionary.items():\n",
    "    i = Counter(items)\n",
    "    string = f\"{le.inverse_transform([key[0]])[0]}-{le.inverse_transform([key[1]])[0]}\"\n",
    "    data[string] = np.array([[i[(0,0)], i[(0,1)]], [i[(1,0)], i[(1,1)]]])\n",
    "\n",
    "fig, axs = plt.subplots(2, 6, figsize=(15, 5))\n",
    "\n",
    "# Loop over each payment type and display the matrix values in the corresponding subplot\n",
    "for i, (payment, matrix) in enumerate(data.items()):\n",
    "    # Compute the row and column indices for the current subplot\n",
    "    row = i // 6\n",
    "    column = i % 6\n",
    "    \n",
    "    # Display the matrix values in the current subplot\n",
    "    axs[row, column].imshow(matrix, cmap='Greens')\n",
    "    axs[row, column].set_xticks([0, 1])\n",
    "    axs[row, column].set_yticks([0, 1])\n",
    "    axs[row, column].set_xticklabels(['0', '1'])\n",
    "    axs[row, column].set_yticklabels(['0', '1'])\n",
    "    axs[row, column].set_xlabel(str(payment.split(\"-\")[1]))\n",
    "    axs[row, column].set_ylabel(str(payment.split(\"-\")[0]))\n",
    "    axs[row, column].xaxis.set_label_position('top')\n",
    "    axs[row, column].yaxis.set_label_position('left')\n",
    "    axs[row, column].xaxis.set_ticks_position('top')\n",
    "    axs[row, column].yaxis.set_ticks_position('left')\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            axs[row, column].annotate(str(matrix[i, j]), xy=(j, i), ha='center', va='center', color='grey')\n",
    "\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.8, hspace=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find circular patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cycles(g: GraphFrame, hop: int = 2):\n",
    "    motif = \"\"\n",
    "\n",
    "    for i in range(hop):\n",
    "        motif += \"(n\" + str(i) + \")-[c\" + str(i+1) + \"]->(n\" + str((i+1) % hop) + \"); \"\n",
    "    motif = motif.strip(\"; \")\n",
    "\n",
    "    filter_string = \"\"\n",
    "    for i in range(hop):\n",
    "        for j in range(i, hop-1):\n",
    "            filter_string += \"n{} != n{}\".format(i, j+1)\n",
    "            if i+1 < hop-1:\n",
    "                filter_string += \" and \"\n",
    "    filter_string += \" and \"\n",
    "    for j in range(1,hop):\n",
    "            filter_string += \"c{}.timestamp < c{}.timestamp\".format(j, j+1)\n",
    "            if(j+1 < hop):\n",
    "                filter_string += \" and \"    \n",
    "    graph = g.find(motif)\n",
    "    graph = graph.filter(filter_string)\n",
    "    select_col = []\n",
    "    for i in range(hop):\n",
    "        select_col.append(\"c{}\".format(i+1))\n",
    "    pattern = np.array(graph.select(*select_col).collect()).squeeze()\n",
    "    total_rows = []\n",
    "\n",
    "    for row in pattern:\n",
    "        if isinstance(row[1], np.ndarray):\n",
    "            for r in row:\n",
    "                #index | timestamp | from | to | payment_format | is_laundering | hop\n",
    "                total_rows.append([int(r[2]), float(r[4]), int(r[0]), int(r[1]), int(r[5]), int(r[6]), hop])\n",
    "        else:\n",
    "            total_rows.append([int(row[2]), float(row[4]), int(row[0]), int(row[1]), int(row[5]), int(row[6]), hop])\n",
    "\n",
    "    dataframe = pd.DataFrame(total_rows, columns=['index', 'timestamp', 'from', 'to', 'payment_format', 'is_laundering', 'hop'])\n",
    "\n",
    "    return dataframe.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = []\n",
    "filtered_spark = df.filter(col(\"payment_currency\") == col(\"receiving_currency\"))\n",
    "filtered_spark.cache()\n",
    "payment_formats = filtered_spark.select(\"payment_format\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "for j in range(1):\n",
    "    verteces = filtered_spark.filter(col(\"payment_format\") == j).select(col(\"from_account\").alias(\"id\")).union(df.select(col(\"to_account\").alias(\"id\"))).distinct()\n",
    "    edges = filtered_spark.filter(col(\"payment_format\") == j).select(col(\"from_account\").alias(\"src\"), col(\"to_account\").alias(\"dst\"), col(\"index\"), col(\"amount_paid\").alias(\"amount\"), col(\"timestamp\"), col(\"payment_format\"), col(\"is_laundering\"))\n",
    "    g = GraphFrame(verteces, edges)\n",
    "    g = g.dropIsolatedVertices()\n",
    "    for i in tqdm.tqdm(range(2,14)):\n",
    "        all_df.append(find_cycles(g, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat(all_df, ignore_index=True)\n",
    "\n",
    "# Step 4: Applica il one-hot encoding sulla colonna \"hop\"\n",
    "one_hot_encoded_df = pd.get_dummies(merged_df, columns=['hop'], prefix='hop')\n",
    "grouped_df = one_hot_encoded_dgroupby('index').agg({\n",
    "    'timestamp': 'first',\n",
    "    'from': 'first',\n",
    "    'to': 'first',\n",
    "    'payment_format': 'first',\n",
    "    'is_laundering': 'first',\n",
    "    'hop_2': 'any',\n",
    "    'hop_3': 'any',\n",
    "    'hop_4': 'any',\n",
    "    'hop_5': 'any',\n",
    "    'hop_6': 'any',\n",
    "    'hop_7': 'any',\n",
    "    'hop_8': 'any',\n",
    "    'hop_9': 'any',\n",
    "    'hop_10': 'any',\n",
    "    'hop_11': 'any',\n",
    "    'hop_12': 'any',\n",
    "}).reset_index()\n",
    "columns_to_encode = ['hop_2', 'hop_3', 'hop_4', 'hop_5', 'hop_6', 'hop_7', 'hop_8', 'hop_9', 'hop_10', 'hop_11', 'hop_12']\n",
    "grouped_df[columns_to_encode] = grouped_df[columns_to_encode].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola il conteggio delle righe che soddisfano le condizioni specificate per ogni valore hop da hop_2 a hop_12\n",
    "hop_columns = grouped_dcolumns[grouped_dcolumns.str.startswith('hop_')].tolist()\n",
    "counts = {}\n",
    "for hop_column in hop_columns:\n",
    "    count_0 = grouped_df[(grouped_df[hop_column] == 1) & (grouped_df['is_laundering'] == 0)].shape[0]\n",
    "    count_1 = grouped_df[(grouped_df[hop_column] == 1) & (grouped_df['is_laundering'] == 1)].shape[0]\n",
    "    counts[hop_column] = {'0': count_0, '1': count_1}\n",
    "\n",
    "# Converti i risultati in un DataFrame\n",
    "counts_df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "counts_df = counts_drename(columns={'index': 'hop'})\n",
    "\n",
    "color_0 = 'green'\n",
    "color_1 = 'orange'\n",
    "\n",
    "# Plot the bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "width = 0.35\n",
    "x = counts_dindex\n",
    "labels = counts_df['hop']\n",
    "bar_0 = ax.bar(x - width / 2, counts_df['0'], width,  label='is_laundering = 0', color=color_0)\n",
    "bar_1 = ax.bar(x + width / 2, counts_df['1'], width,  label='is_laundering = 1', color=color_1)\n",
    "\n",
    "ax.set_xlabel('Hop Values')\n",
    "ax.set_ylabel('Number of Rows')\n",
    "ax.set_title('Count of Rows with Hop Values and is_laundering')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "# Add count labels above each bar\n",
    "def autolabel(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "        \n",
    "autolabel(bar_0)\n",
    "autolabel(bar_1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Fan In "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fanin(g: GraphFrame):\n",
    "    motif = \"(a)-[c1]->(b); (c)-[c2]->(b)\"\n",
    "    filter_motif = \"(abs(c1.timestamp - c2.timestamp)) <= 40000 and c1.index != c2.index and c1.payment_currency == c2.payment_currency\"#and c1.payment_format == c2.payment_format\"\n",
    "  \n",
    "    pattern = g.find(motif).filter(filter_motif).select(\"c1\", \"c2\").distinct()\n",
    "    fan_in_trans = pattern.groupBy(col(\"c1\")).agg(count(\"*\").alias(\"fan_in_degree\")).select(col(\"c1\").alias(\"transaction\"), col(\"fan_in_degree\"))\n",
    "    fan_in_trans.cache()\n",
    "    \n",
    "    \n",
    "        \n",
    "    return fan_in_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_spark = spark_dfilter(col(\"payment_currency\") == col(\"receiving_currency\"))\n",
    "filtered_spark.cache()\n",
    "payment_formats = filtered_spark.select(\"payment_format\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "total_fan_in = None\n",
    "\n",
    "for payment_format in payment_formats:\n",
    "    print(f\"Find fan in payment_format: {payment_format}\")\n",
    "    filtered_by_format = filtered_spark.filter(col(\"payment_format\") == payment_format)\n",
    "    verteces = (\n",
    "        filtered_by_format.select(col(\"from_account\").alias(\"id\"))\n",
    "        .union(spark_df.select(col(\"to_account\").alias(\"id\")))\n",
    "        .distinct()\n",
    "    )\n",
    "    edges = (\n",
    "        filtered_by_format.select(\n",
    "            col(\"from_account\").alias(\"src\"),\n",
    "            col(\"to_account\").alias(\"dst\"),\n",
    "            col(\"index\"),\n",
    "            col(\"timestamp\"),\n",
    "            col(\"payment_currency\"),\n",
    "            col(\"payment_format\"),\n",
    "            col(\"is_laundering\")\n",
    "            \n",
    "        )\n",
    "    )\n",
    "    g = GraphFrame(verteces, edges)\n",
    "    if total_fan_in is None:\n",
    "        total_fan_in = find_fanin(g)\n",
    "    else:\n",
    "        total_fan_in = total_fan_in.unionAll(find_fanin(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci la funzione UDF per estrarre i valori dalla colonna \"transaction\" e creare una struttura\n",
    "def extract_values(transaction):\n",
    "    src, dst, index, timestamp, payment_currency, payment_format, is_laundering = transaction\n",
    "    return (src, dst, index, timestamp,payment_currency,  payment_format, is_laundering)\n",
    "\n",
    "# Definisci lo schema per il DataFrame Spark\n",
    "schema = StructType([\n",
    "    StructField(\"src\", IntegerType(), True),\n",
    "    StructField(\"dst\", IntegerType(), True),\n",
    "    StructField(\"index\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", FloatType(), True),\n",
    "    StructField(\"payment_currency\", IntegerType(), True),\n",
    "    StructField(\"payment_format\", IntegerType(), True),\n",
    "    StructField(\"is_laundering\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applica la funzione UDF per estrarre i valori dalla colonna \"transaction\" e crea un nuovo DataFrame\n",
    "extract_udf = udf(extract_values, schema)\n",
    "new_spark_df = total_fan_in.withColumn(\"extracted\", extract_udf(\"transaction\"))\n",
    "\n",
    "# Seleziona le colonne necessarie e converte il DataFrame Spark in un DataFrame Pandas\n",
    "pandas_df = new_spark_df.select(\"extracted.*\", \"fan_in_degree\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Assume you have loaded the DataFrame 'df' with the data\n",
    "\n",
    "# Filter out rows where timestamp is NaN (if needed)\n",
    "# df = ddropna(subset=['timestamp'])\n",
    "\n",
    "# Create a color map for different payment_formats\n",
    "num_unique_payment_formats = pandas_df['payment_format'].nunique()\n",
    "color_map = cm.get_cmap('tab20', num_unique_payment_formats) # Choose 'tab20' colormap for more distinct colors\n",
    "\n",
    "# Mapping dictionary for payment_format names\n",
    "payment_format_names = {\n",
    "    0: 'ACH',\n",
    "    1: 'Bitcoin',\n",
    "    2: 'Cash',\n",
    "    3: 'Cheque',\n",
    "    4: 'Credit Card',\n",
    "    5: 'Reinvestment',\n",
    "    6: 'Wire',\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "# Replace the payment_format values with their desired names\n",
    "pandas_df['payment_format_name'] = pandas_df['payment_format'].map(payment_format_names)\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = plt.figure(figsize=(25, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the points with different colors based on payment_format\n",
    "for i, (payment_format, group_df) in enumerate(pandas_dgroupby('payment_format_name')):\n",
    "    color = color_map(i / num_unique_payment_formats)  # Map payment_format to a color from the colormap\n",
    "    ax.scatter(group_df['fan_in_degree'], group_df['is_laundering'], group_df['payment_format'],\n",
    "               alpha=0.5, marker='o', color=color, label=f'Payment_format={payment_format}')\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('Fan_in_degree')\n",
    "ax.set_ylabel('Is_laundering')\n",
    "ax.set_zlabel('Payment_format')\n",
    "\n",
    "# Set the title\n",
    "ax.set_title('Correlation between Fan_in_degree, Is_laundering, and Payment_format')\n",
    "\n",
    "# Create a custom legend outside of the 3D plot\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find fan out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_fanout(g: GraphFrame):\n",
    "    motif = \"(a)-[c1]->(b); (a)-[c2]->(c)\"\n",
    "    filter_motif = \"(abs(c1.timestamp - c2.timestamp)) <= 40000 and c1.index != c2.index\"\n",
    "  \n",
    "    pattern = g.find(motif).filter(filter_motif).select(\"c1\", \"c2\").distinct()\n",
    "    fan_out_trans = pattern.groupBy(col(\"c1\")).agg(count(\"*\").alias(\"fan_out_degree\")).select(col(\"c1\").alias(\"transaction\"), col(\"fan_out_degree\"))\n",
    "    fan_out_trans.cache()\n",
    "    \n",
    "    \n",
    "        \n",
    "    return fan_out_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_spark = spark_dfilter(col(\"payment_currency\") == col(\"receiving_currency\"))\n",
    "filtered_spark.cache()\n",
    "payment_formats = filtered_spark.select(\"payment_format\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "total_fan_out = None\n",
    "\n",
    "for payment_format in range(2):\n",
    "    print(f\"Find fan in payment_format: {payment_format}\")\n",
    "    filtered_by_format = filtered_spark.filter(col(\"payment_format\") == payment_format)\n",
    "    verteces = (\n",
    "        filtered_by_format.select(col(\"from_account\").alias(\"id\"))\n",
    "        .union(spark_df.select(col(\"to_account\").alias(\"id\")))\n",
    "        .distinct()\n",
    "    )\n",
    "    edges = (\n",
    "        filtered_by_format.select(\n",
    "            col(\"from_account\").alias(\"src\"),\n",
    "            col(\"to_account\").alias(\"dst\"),\n",
    "            col(\"index\"),\n",
    "            col(\"timestamp\"),\n",
    "            col(\"payment_format\"),\n",
    "            col(\"is_laundering\")\n",
    "        )\n",
    "    )\n",
    "    g = GraphFrame(verteces, edges)\n",
    "    if total_fan_out is None:\n",
    "        total_fan_out = find_fanout(g)\n",
    "    else:\n",
    "        total_fan_out = total_fan_out.unionAll(find_fanout(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci la funzione UDF per estrarre i valori dalla colonna \"transaction\" e creare una struttura\n",
    "def extract_values(transaction):\n",
    "    src, dst, index, timestamp, payment_format, is_laundering = transaction\n",
    "    return (src, dst, index, timestamp, payment_format, is_laundering)\n",
    "\n",
    "# Definisci lo schema per il DataFrame Spark\n",
    "schema = StructType([\n",
    "    StructField(\"src\", IntegerType(), True),\n",
    "    StructField(\"dst\", IntegerType(), True),\n",
    "    StructField(\"index\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", FloatType(), True),\n",
    "    StructField(\"payment_format\", IntegerType(), True),\n",
    "    StructField(\"is_laundering\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applica la funzione UDF per estrarre i valori dalla colonna \"transaction\" e crea un nuovo DataFrame\n",
    "extract_udf = udf(extract_values, schema)\n",
    "new_spark_df = total_fan_out.withColumn(\"extracted\", extract_udf(\"transaction\"))\n",
    "\n",
    "# Seleziona le colonne necessarie e converte il DataFrame Spark in un DataFrame Pandas\n",
    "pandas_df = new_spark_df.select(\"extracted.*\", \"fan_out_degree\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "num_unique_payment_formats = pandas_df['payment_format'].nunique()\n",
    "color_map = cm.get_cmap('tab20', num_unique_payment_formats) \n",
    "\n",
    "# Mapping dictionary for payment_format names\n",
    "payment_format_names = {\n",
    "    0: 'ACH',\n",
    "    1: 'Bitcoin'\n",
    "}\n",
    "\n",
    "# Replace the payment_format values with their desired names\n",
    "pandas_df['payment_format_name'] = pandas_df['payment_format'].map(payment_format_names)\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = plt.figure(figsize=(25, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the points with different colors based on payment_format\n",
    "for i, (payment_format, group_df) in enumerate(pandas_dgroupby('payment_format_name')):\n",
    "    color = color_map(i / num_unique_payment_formats)  # Map payment_format to a color from the colormap\n",
    "    ax.scatter(group_df['fan_out_degree'], group_df['is_laundering'], group_df['payment_format'],\n",
    "               alpha=0.5, marker='o', color=color, label=f'Payment_format={payment_format}')\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('Fan_out_degree')\n",
    "ax.set_ylabel('Is_laundering')\n",
    "ax.set_zlabel('Payment_format')\n",
    "\n",
    "# Set the title\n",
    "ax.set_title('Correlation between Fan_out_degree, Is_laundering, and Payment_format')\n",
    "\n",
    "# Create a custom legend outside of the 3D plot\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

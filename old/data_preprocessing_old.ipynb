{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import tqdm\n",
    "from pyspark.sql.window import Window\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from graphframes import GraphFrame\n",
    "from pyspark.sql.types import *\n",
    "import multiprocessing\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset/HI-Small_Trans.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_driver_memory = \"8g\"\n",
    "spark_executor_memory = \"4g\"\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(\"spark.driver.memory\", spark_driver_memory) \\\n",
    "                    .config(\"spark.executor.memory\", spark_executor_memory) \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .getOrCreate()\n",
    "print(\"Spark session created\")\n",
    "sc = spark.sparkContext\n",
    "print(\"Spark context created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.parquet('../project/df.parquet').drop(\"__index_level_0__\")\n",
    "spark_df = spark_df.withColumn(\"timestamp\", date_format(\"timestamp\", \"yyyyMMddHHmm\").cast(LongType()))\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.persist()\n",
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset train and test\n",
    "sorted_df = spark_df.orderBy(\"timestamp\")\n",
    "\n",
    "# Calcola il numero di righe per l'addestramento e la convalida\n",
    "total_rows = sorted_df.count()\n",
    "train_percentage = 0.8\n",
    "train_rows = int(total_rows * train_percentage)\n",
    "validation_rows = total_rows - train_rows\n",
    "\n",
    "# Dividi il DataFrame in modo sequenziale 80/20\n",
    "train_df = sorted_df.limit(train_rows)\n",
    "validation_df = sorted_df.subtract(train_df)\n",
    "\n",
    "# Assegna un indice univoco a ciascuna riga\n",
    "train_df = train_df.repartition(1)\n",
    "train_df = train_df.withColumn(\"index\", F.monotonically_increasing_id())\n",
    "train_df = train_df.repartition(12)\n",
    "validation_df = validation_df.repartition(1)\n",
    "validation_df = validation_df.withColumn(\"index\", F.monotonically_increasing_id())\n",
    "validation_df = validation_df.repartition(12)\n",
    "\n",
    "# Stampa il numero di righe in ciascun DataFrame\n",
    "#print(\"Number of records in training DataFrame: \", train_df.count())\n",
    "#print(\"Number of records in validation DataFrame: \", validation_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_and_load_dataframe(df, name):\n",
    "    df.write\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(f\"./preprocessed_data/{name}_small\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find transactions that occur one after the other with the same amount and currency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = {}\n",
    "values = list(sorted_df.select(\"payment_format\").distinct().collect())\n",
    "j = 0\n",
    "for i in range(len(values)):\n",
    "    for k in range(0, len(values)):\n",
    "        combinations[(i,k)] = j\n",
    "        j+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_money_send_to_send(df):\n",
    "\n",
    "    train_vertices = df.select(F.col(\"from_account\").alias(\"id\")).union(df.select(F.col(\"to_account\").alias(\"id\"))).distinct()\n",
    "    train_edges = df.select(F.col(\"from_account\").alias(\"src\"), F.col(\"to_account\").alias(\"dst\"), F.col(\"index\"), F.col(\"amount_paid\").alias(\"amount\"), F.col(\"timestamp\"), F.col(\"payment_format\"), F.col(\"is_laundering\"))\n",
    "    g = GraphFrame(train_vertices, train_edges)\n",
    "\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"index\", LongType(), False),\n",
    "        StructField(\"timestamp\", DoubleType(), False),\n",
    "        StructField(\"from\", IntegerType(), False),\n",
    "        StructField(\"to\", IntegerType(), False),\n",
    "        StructField(\"payment_format\", IntegerType(), False),\n",
    "        StructField(\"is_laundering\", IntegerType(), False),\n",
    "        StructField(\"payment_payment\", IntegerType(), False)\n",
    "    ])\n",
    "\n",
    "\n",
    "    motif = \"(a)-[c1]->(b); (b)-[c2]->(c)\"\n",
    "    filter_string = \"a != b and b != c and c1.amount == c2.amount and c1.timestamp < c2.timestamp\"\n",
    "    graph = g.find(motif).filter(filter_string).distinct()\n",
    "    graph.cache()\n",
    "    columns = ['c1', 'c2']\n",
    "    pattern = np.array(graph.select(*columns).collect()).squeeze()\n",
    "    total_rows = []\n",
    "\n",
    "    for row in pattern:\n",
    "        rows_to_append = []\n",
    "        payment_formats = []\n",
    "        if isinstance(row[1], np.ndarray):\n",
    "            for r in row:\n",
    "                #index | timestamp | from | to | payment_format | is_laundering \n",
    "                rows_to_append.append([int(r[2]), float(r[4]), int(r[0]), int(r[1]), int(r[5]), int(r[6])])\n",
    "                payment_formats.append(int(r[5]))\n",
    "        else:\n",
    "                rows_to_append.append([int(r[2]), float(r[4]), int(r[0]), int(r[1]), int(r[5]), int(r[6])])\n",
    "                payment_formats.append(int(r[5]))\n",
    "        \n",
    "        for r in rows_to_append:\n",
    "            r.append(combinations[(payment_formats[0], payment_formats[1])])\n",
    "            total_rows.append(r)\n",
    "\n",
    "    temp_df = spark.createDataFrame(total_rows, schema)\n",
    "\n",
    "    temp_df = temp_df.dropDuplicates(['index'])\n",
    "\n",
    "    joined_df = df.join(temp_df.select(\"index\", \"payment_payment\").withColumnRenamed(\"payment_payment\", \"payment_payment_B\"), on=\"index\", how=\"left\")\n",
    "\n",
    "    # Aggiungi la colonna \"payment_payment\" a dfA, usando il valore corrispondente da dfB se presente, altrimenti imposta -1\n",
    "    df = joined_df.withColumn(\"payment_payment\", F.when(F.col(\"payment_payment_B\").isNotNull(), F.col(\"payment_payment_B\")).otherwise(-1)).drop(\"payment_payment_B\")\n",
    "  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = add_money_send_to_send(train_df)\n",
    "validation_df = add_money_send_to_send(validation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find circular patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cycles(df):\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"index\", LongType(), False),\n",
    "        StructField(\"timestamp\", DoubleType(), False),\n",
    "        StructField(\"from\", IntegerType(), False),\n",
    "        StructField(\"to\", IntegerType(), False),\n",
    "        StructField(\"payment_format\", IntegerType(), False),\n",
    "        StructField(\"is_laundering\", IntegerType(), False),\n",
    "        StructField(\"hop_2\", IntegerType(), False),\n",
    "        StructField(\"hop_3\", IntegerType(), False),\n",
    "        StructField(\"hop_4\", IntegerType(), False),\n",
    "        StructField(\"hop_5\", IntegerType(), False),\n",
    "        StructField(\"hop_6\", IntegerType(), False),\n",
    "        StructField(\"hop_7\", IntegerType(), False),\n",
    "        StructField(\"hop_8\", IntegerType(), False),\n",
    "        StructField(\"hop_9\", IntegerType(), False),\n",
    "        StructField(\"hop_10\", IntegerType(), False),\n",
    "        StructField(\"hop_11\", IntegerType(), False),\n",
    "        StructField(\"hop_12\", IntegerType(), False),\n",
    "        StructField(\"hop_13\", IntegerType(), False),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    all_df = []\n",
    "    filtered_spark = df.filter(F.col(\"payment_currency\") == F.col(\"receiving_currency\"))\n",
    "    filtered_spark.cache()\n",
    "    #payment_formats = filtered_spark.select(\"payment_format\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    for j in range(1):\n",
    "        verteces = filtered_spark.filter(F.col(\"payment_format\") == j).select(F.col(\"from_account\").alias(\"id\")).union(spark_df.select(F.col(\"to_account\").alias(\"id\"))).distinct()\n",
    "        edges = filtered_spark.filter(F.col(\"payment_format\") == j).select(F.col(\"from_account\").alias(\"src\"), F.col(\"to_account\").alias(\"dst\"), F.col(\"index\"), F.col(\"amount_paid\").alias(\"amount\"), F.col(\"timestamp\"), F.col(\"payment_format\"), F.col(\"is_laundering\"))\n",
    "        g = GraphFrame(verteces, edges)\n",
    "        g = g.dropIsolatedVertices()\n",
    "        g.cache()\n",
    "        for hop in tqdm.tqdm(range(2,15)):\n",
    "            motif = \"\"\n",
    "\n",
    "            for i in range(hop):\n",
    "                motif += \"(n\" + str(i) + \")-[c\" + str(i+1) + \"]->(n\" + str((i+1) % hop) + \"); \"\n",
    "            motif = motif.strip(\"; \")\n",
    "\n",
    "            filter_string = \"\"\n",
    "            for i in range(hop):\n",
    "                for j in range(i, hop-1):\n",
    "                    filter_string += \"n{} != n{}\".format(i, j+1)\n",
    "                    if i+1 < hop-1:\n",
    "                        filter_string += \" and \"\n",
    "            filter_string += \" and \"\n",
    "            for j in range(1,hop):\n",
    "                    filter_string += \"c{}.timestamp < c{}.timestamp\".format(j, j+1)\n",
    "                    if(j+1 < hop):\n",
    "                        filter_string += \" and \"    \n",
    "            graph = g.find(motif)\n",
    "            graph = graph.filter(filter_string)\n",
    "            select_col = []\n",
    "            for i in range(hop):\n",
    "                select_col.append(\"c{}\".format(i+1))\n",
    "            pattern = np.array(graph.select(*select_col).collect()).squeeze()\n",
    "            total_rows = []\n",
    "\n",
    "            for row in pattern:\n",
    "                if isinstance(row[1], np.ndarray):\n",
    "                    for r in row:\n",
    "                        #index | timestamp | from | to | payment_format | is_laundering | hop\n",
    "                        total_rows.append([int(r[2]), r[4], int(r[0]), int(r[1]), int(r[5]), int(r[6]), hop])\n",
    "                else:\n",
    "                    total_rows.append([int(row[2]), row[4], int(row[0]), int(row[1]), int(row[5]), int(row[6]), hop])\n",
    "\n",
    "            dataframe = pd.DataFrame(total_rows, columns=['index', 'timestamp', 'from', 'to', 'payment_format', 'is_laundering', 'hop'])\n",
    "\n",
    "            all_df.append(dataframe.drop_duplicates())\n",
    "\n",
    "    merged_df = pd.concat(all_df, ignore_index=True)\n",
    "    one_hot_encoded_df = pd.get_dummies(merged_df, columns=['hop'], prefix='hop')\n",
    "\n",
    "    # Manually add missing hop columns (hop_2 to hop_13) and fill with False\n",
    "    columns_to_add = [f\"hop_{i}\" for i in range(2, 14)]\n",
    "    for col in columns_to_add:\n",
    "        if col not in one_hot_encoded_df.columns:\n",
    "            one_hot_encoded_df[col] = False\n",
    "\n",
    "    grouped_df = one_hot_encoded_df.groupby('index').agg({\n",
    "        'timestamp': 'first',\n",
    "        'from': 'first',\n",
    "        'to': 'first',\n",
    "        'payment_format': 'first',\n",
    "        'is_laundering': 'first',\n",
    "        **{col: 'any' for col in columns_to_add}\n",
    "    }).reset_index()\n",
    "   \n",
    "    columns_to_encode = ['hop_2', 'hop_3', 'hop_4', 'hop_5', 'hop_6', 'hop_7', 'hop_8', 'hop_9', 'hop_10', 'hop_11', 'hop_12', 'hop_13']\n",
    "    grouped_df[columns_to_encode] = grouped_df[columns_to_encode].fillna(False, inplace=False).astype(int)\n",
    "    \n",
    "    temp_df = spark.createDataFrame(grouped_df, schema)\n",
    "\n",
    "    temp_df = temp_df.dropDuplicates(['index'])\n",
    "\n",
    "    # Step 1: Seleziona solo le colonne necessarie da temp_df e rinomina le colonne\n",
    "    temp_df_selected = temp_df.select(\n",
    "        \"index\",\n",
    "        \"hop_2\", \"hop_3\", \"hop_4\", \"hop_5\", \"hop_6\",\n",
    "        \"hop_7\", \"hop_8\", \"hop_9\", \"hop_10\", \"hop_11\",\n",
    "        \"hop_12\", \"hop_13\"\n",
    "    ).withColumnRenamed(\"hop_2\", \"hop_2_B\").withColumnRenamed(\"hop_3\", \"hop_3_B\").withColumnRenamed(\"hop_4\", \"hop_4_B\").withColumnRenamed(\"hop_5\", \"hop_5_B\").withColumnRenamed(\"hop_6\", \"hop_6_B\").withColumnRenamed(\"hop_7\", \"hop_7_B\").withColumnRenamed(\"hop_8\", \"hop_8_B\").withColumnRenamed(\"hop_9\", \"hop_9_B\").withColumnRenamed(\"hop_10\", \"hop_10_B\").withColumnRenamed(\"hop_11\", \"hop_11_B\").withColumnRenamed(\"hop_12\", \"hop_12_B\").withColumnRenamed(\"hop_13\", \"hop_13_B\")\n",
    "\n",
    "    # Step 2: Esegui una left join tra df e temp_df_selected, usando l'indice come chiave di join\n",
    "    joined_df = df.join(temp_df_selected, on=\"index\", how=\"left\")\n",
    "\n",
    "    # Step 3: Usa la funzione when per assegnare il valore corrispondente da hop_i_B se presente, altrimenti imposta il valore a 0\n",
    "    for i in range(2, 14):\n",
    "        joined_df = joined_df.withColumn(\n",
    "            f\"hop_{i}\", \n",
    "            F.when(F.col(f\"hop_{i}_B\").isNotNull(), F.col(f\"hop_{i}_B\")).otherwise(0)\n",
    "        )\n",
    "\n",
    "    # Step 4: Rimuovi le colonne aggiunte da temp_df_selected\n",
    "    joined_df = joined_df.drop(*[f\"hop_{i}_B\" for i in range(2, 14)])\n",
    "\n",
    "    # Il risultato finale è il dataframe df con le colonne hop_2 a hop_13 aggiunte e i valori 0 dove necessario\n",
    "    df_result = joined_df\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = find_cycles(train_df)\n",
    "validation_df = find_cycles(validation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Fan in degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fanin(g: GraphFrame):\n",
    "    motif = \"(a)-[c1]->(b); (c)-[c2]->(b)\"\n",
    "    filter_motif = \"(abs(c1.timestamp - c2.timestamp)) <= 40000 and c1.index != c2.index and c1.payment_currency == c2.payment_currency\"#and c1.payment_format == c2.payment_format\"\n",
    "  \n",
    "    pattern = g.find(motif).filter(filter_motif).select(\"c1\", \"c2\").distinct()\n",
    "    fan_in_trans = pattern.groupBy(F.col(\"c1\")).agg(F.count(\"*\").alias(\"fan_in_degree\")).select(F.col(\"c1\").alias(\"transaction\"), F.col(\"fan_in_degree\"))\n",
    "    #fan_in_trans.cache()\n",
    "    return fan_in_trans\n",
    "\n",
    "def add_fan_in(df):\n",
    "    filtered_spark = df.filter(F.col(\"payment_currency\") == F.col(\"receiving_currency\"))\n",
    "    filtered_spark.cache()\n",
    "    payment_formats = filtered_spark.select(\"payment_format\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    total_fan_in = None\n",
    "\n",
    "    for payment_format in payment_formats:\n",
    "        print(f\"Find fan in payment_format: {payment_format}\")\n",
    "        filtered_by_format = filtered_spark.filter(F.col(\"payment_format\") == payment_format)\n",
    "        verteces = (\n",
    "            filtered_by_format.select(F.col(\"from_account\").alias(\"id\"))\n",
    "            .union(spark_df.select(F.col(\"to_account\").alias(\"id\")))\n",
    "            .distinct()\n",
    "        )\n",
    "        edges = (\n",
    "            filtered_by_format.select(\n",
    "                F.col(\"from_account\").alias(\"src\"),\n",
    "                F.col(\"to_account\").alias(\"dst\"),\n",
    "                F.col(\"index\"),\n",
    "                F.col(\"timestamp\"),\n",
    "                F.col(\"payment_currency\"),\n",
    "                F.col(\"payment_format\"),\n",
    "                F.col(\"is_laundering\")\n",
    "                \n",
    "            )\n",
    "        )\n",
    "        g = GraphFrame(verteces, edges)\n",
    "        if total_fan_in is None:\n",
    "            total_fan_in = find_fanin(g)\n",
    "        else:\n",
    "            total_fan_in = total_fan_in.unionAll(find_fanin(g))    \n",
    "    \n",
    "    \n",
    "    def extract_values(transaction):\n",
    "        src, dst, index, timestamp, payment_currency, payment_format, is_laundering = transaction\n",
    "        return (src, dst, index, timestamp,payment_currency,  payment_format, is_laundering)\n",
    "\n",
    "    # Definisci lo schema per il DataFrame Spark\n",
    "    schema_udf = StructType([\n",
    "        StructField(\"src\", IntegerType(), True),\n",
    "        StructField(\"dst\", IntegerType(), True),\n",
    "        StructField(\"index\", IntegerType(), True),\n",
    "        StructField(\"timestamp\", FloatType(), True),\n",
    "        StructField(\"payment_currency\", IntegerType(), True),\n",
    "        StructField(\"payment_format\", IntegerType(), True),\n",
    "        StructField(\"is_laundering\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    # Applica la funzione UDF per estrarre i valori dalla colonna \"transaction\" e crea un nuovo DataFrame\n",
    "    extract_udf = F.udf(extract_values, schema_udf)\n",
    "    new_spark_df = total_fan_in.withColumn(\"extracted\", extract_udf(\"transaction\"))\n",
    "\n",
    "    # Seleziona le colonne necessarie e converte il DataFrame Spark in un DataFrame Pandas\n",
    "    temp_df = new_spark_df.select(\"extracted.*\", \"fan_in_degree\")\n",
    "\n",
    "    joined_df = df.join(temp_df.select(\"index\", \"fan_in_degree\").withColumnRenamed(\"fan_in_degree\", \"fan_in_degree_B\"), on=\"index\", how=\"left\")\n",
    "\n",
    "    # Aggiungi la colonna \"payment_payment\" a dfA, usando il valore corrispondente da dfB se presente, altrimenti imposta -1\n",
    "    df = joined_df.withColumn(\"fan_in_degree\", F.when(F.col(\"fan_in_degree_B\").isNotNull(), F.col(\"fan_in_degree_B\")).otherwise(0)).drop(\"fan_in_degree_B\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = add_fan_in(train_df)\n",
    "validation_df = add_fan_in(validation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Fan out degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fanout(g: GraphFrame):\n",
    "    motif = \"(a)-[c1]->(b); (a)-[c2]->(c)\"\n",
    "    filter_motif = \"(abs(c1.timestamp - c2.timestamp)) <= 40000 and c1.index != c2.index\"\n",
    "  \n",
    "    pattern = g.find(motif).filter(filter_motif).select(\"c1\", \"c2\").distinct()\n",
    "    fan_out_trans = pattern.groupBy(F.col(\"c1\")).agg(F.count(\"*\").alias(\"fan_out_degree\")).select(F.col(\"c1\").alias(\"transaction\"), F.col(\"fan_out_degree\"))\n",
    "    fan_out_trans.cache()\n",
    "    \n",
    "    return fan_out_trans\n",
    "\n",
    "def add_fan_out(df):\n",
    "    filtered_spark = df.filter(F.col(\"payment_currency\") == F.col(\"receiving_currency\"))\n",
    "    filtered_spark.cache()\n",
    "    #payment_formats = filtered_spark.select(\"payment_format\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    total_fan_out = None\n",
    "\n",
    "    for payment_format in range(2):\n",
    "        print(f\"Find fan in payment_format: {payment_format}\")\n",
    "        filtered_by_format = filtered_spark.filter(F.col(\"payment_format\") == payment_format)\n",
    "        verteces = (\n",
    "            filtered_by_format.select(F.col(\"from_account\").alias(\"id\"))\n",
    "            .union(spark_df.select(F.col(\"to_account\").alias(\"id\")))\n",
    "            .distinct()\n",
    "        )\n",
    "        edges = (\n",
    "            filtered_by_format.select(\n",
    "                F.col(\"from_account\").alias(\"src\"),\n",
    "                F.col(\"to_account\").alias(\"dst\"),\n",
    "                F.col(\"index\"),\n",
    "                F.col(\"timestamp\"),\n",
    "                F.col(\"payment_format\"),\n",
    "                F.col(\"is_laundering\")\n",
    "            )\n",
    "        )\n",
    "        g = GraphFrame(verteces, edges)\n",
    "        if total_fan_out is None:\n",
    "            total_fan_out = find_fanout(g)\n",
    "        else:\n",
    "            total_fan_out = total_fan_out.unionAll(find_fanout(g))    \n",
    "    \n",
    "    \n",
    "    def extract_values(transaction):\n",
    "        src, dst, index, timestamp,  payment_format, is_laundering = transaction\n",
    "        return (src, dst, index, timestamp, payment_format, is_laundering)\n",
    "\n",
    "    # Definisci lo schema per il DataFrame Spark\n",
    "    schema_udf = StructType([\n",
    "        StructField(\"src\", IntegerType(), True),\n",
    "        StructField(\"dst\", IntegerType(), True),\n",
    "        StructField(\"index\", IntegerType(), True),\n",
    "        StructField(\"timestamp\", FloatType(), True),\n",
    "        StructField(\"payment_format\", IntegerType(), True),\n",
    "        StructField(\"is_laundering\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    # Applica la funzione UDF per estrarre i valori dalla colonna \"transaction\" e crea un nuovo DataFrame\n",
    "    extract_udf = F.udf(extract_values, schema_udf)\n",
    "    new_spark_df = total_fan_out.withColumn(\"extracted\", extract_udf(\"transaction\"))\n",
    "\n",
    "    # Seleziona le colonne necessarie e converte il DataFrame Spark in un DataFrame Pandas\n",
    "    temp_df = new_spark_df.select(\"extracted.*\", \"fan_out_degree\")\n",
    "\n",
    "    joined_df = df.join(temp_df.select(\"index\", \"fan_out_degree\").withColumnRenamed(\"fan_out_degree\", \"fan_out_degree_B\"), on=\"index\", how=\"left\")\n",
    "\n",
    "    # Aggiungi la colonna \"payment_payment\" a dfA, usando il valore corrispondente da dfB se presente, altrimenti imposta -1\n",
    "    df = joined_df.withColumn(\"fan_out_degree\", F.when(F.col(\"fan_out_degree_B\").isNotNull(), F.col(\"fan_out_degree_B\")).otherwise(0)).drop(\"fan_out_degree_B\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = add_fan_out(train_df)\n",
    "validation_df = add_fan_out(validation_df)\n",
    "\n",
    "train_df = write_and_load_dataframe(train_df, \"train\")\n",
    "validation_df = write_and_load_dataframe(validation_df, \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_dataframe(df):\n",
    "    return df\\\n",
    "    .withColumn(\"same_account\", F.when(F.col(\"from_account\") == F.col(\"to_account\"), 1).otherwise(0))\\\n",
    "    .withColumn(\"same_currency\", F.when(F.col(\"receiving_currency\") == F.col(\"payment_currency\"), 1).otherwise(0))\\\n",
    "    .withColumn(\"same_bank\", F.when(F.col(\"from_bank\") == F.col(\"to_bank\"), 1).otherwise(0))\\\n",
    "    .withColumn(\"same_amount\", F.when(F.col(\"amount_received\") == F.col(\"amount_paid\"), 1).otherwise(0))\\\n",
    "    .drop(F.col(\"index\"))\\\n",
    "    .drop(F.col(\"timestamp\"))\\\n",
    "    .drop(F.col(\"from_bank\"))\\\n",
    "    .drop(F.col(\"to_bank\"))\\\n",
    "    .drop(F.col(\"from_account\"))\\\n",
    "    .drop(F.col(\"to_account\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_final_dataframe(train_df)\n",
    "validation_df = create_final_dataframe(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(\"from_account\").drop(\"to_account\")\n",
    "validation_df = validation_df.drop(\"from_account\").drop(\"to_account\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = write_and_load_dataframe(train_df, \"train\")\n",
    "validation_df = write_and_load_dataframe(validation_df, \"validation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

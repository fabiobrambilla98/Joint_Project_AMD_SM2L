{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/fabio/jars\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from pyspark.sql.window import Window\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from graphframes import GraphFrame\n",
    "from pyspark.sql.types import *\n",
    "import multiprocessing\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_driver_memory = \"10g\"\n",
    "spark_executor_memory = \"6g\"\n",
    "spark_partial_results_folder = './partial_results'\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(\"spark.driver.memory\", spark_driver_memory) \\\n",
    "                    .config(\"spark.executor.memory\", spark_executor_memory) \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .config(\"spark.sql.autoBroadcastJoinThreshold\", 100 * 1024 * 1024)\\\n",
    "                    .config(\"spark.driver.port\", 4040) \\\n",
    "                    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "                    .getOrCreate()\n",
    "print(\"Spark session created\")\n",
    "sc = spark.sparkContext\n",
    "print(\"Spark context created\")\n",
    "\n",
    "\n",
    "if not os.path.exists(spark_partial_results_folder):\n",
    "    os.makedirs(spark_partial_results_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione dello schema per la lettura del file \n",
    "schema = StructType([\n",
    "    StructField('timestamp', StringType(), True),\n",
    "    StructField('from_bank', IntegerType(), True),\n",
    "    StructField('from_account', StringType(), True),\n",
    "    StructField('to_bank', IntegerType(), True),\n",
    "    StructField('to_account', StringType(), True),\n",
    "    StructField('amount_received', FloatType(), True),\n",
    "    StructField('receiving_currency', StringType(), True),\n",
    "    StructField('amount_paid', FloatType(), True),\n",
    "    StructField('payment_currency', StringType(), True),\n",
    "    StructField('payment_format', StringType(), True),\n",
    "    StructField('is_laundering', IntegerType(), True)])\n",
    "\n",
    "#Lettura del file csv direttamente in spark\n",
    "df = spark.read.csv(\"../dataset/HI-Small_Trans.csv\", header = False, schema=schema)\n",
    "\n",
    "#Rimozione della prima riga in quanto è presente l'header del csv\n",
    "df = df.filter(col('timestamp') != \"Timestamp\")\n",
    "\n",
    "#Aggiunta dell'id univoco per le righe.\n",
    "df = df.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "df.show(5, truncate=False)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'analisi sui dati verrà suddivisa in più parti: \n",
    "Inizialmente verrà fatta un'analisi indipendente dall'ordine temporale, che si concentrerà principalmente sul capire la struttura del dataset, le proporzioni tra laundering e non laundering e se ci sono marcate differenze tra le features delle due classi. \n",
    "Successivamente si andrà ad esplorare meglio la relazione temporale tra i dati essendo il tempo una delle feature principali quando si cerca di scovare pattern fraudolenti.\n",
    "Infine, calcolate le features che si pensa possano essere di interesse, verrà calcolata la matrice di correlazione per capire quanto siano correlate le features all'interno del dataset per capirne anche l'importanza ai fini dell'apprendimento del modello."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prime analisi sul dataset\n",
    "\n",
    "In questa sezione andremo a vedere come sono distribuite le classi all'interno del dataset, andando a studiare anche la proporzione di transazioni fraudolente per le diverse tipologie di metodi di pagamento e di formato, oltre alla differenza, se presente, tra amount che vengono spostati quando le transazioni sono fraudolente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controllo se ci sono valori nulli in qualche cella\n",
    "df.select([F.sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non sono presenti valori nulli, quindi non è necessario rimuovere o modificare alcuna riga. Dunque si può procedere con le successive analisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proporzione tra laundering e non laundering all'interno del dataset. 1: Laundering, 0: Not laundering\n",
    "total_count = df.count()\n",
    "df.select('is_laundering').groupBy('is_laundering').agg(count('*').alias('count')).withColumn(\"proportion\", col('count')/total_count).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fin da subito è possibile osservare che le transazioni fraudolente sono all'incirca 1/1000 di quelle totali. Questo porta ad avere un dataset altamente sbilanciato e rende l'identificazione di pattern più complessa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display payment format in relation to laundering transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('payment_format', 'is_laundering') \\\n",
    "    .groupBy('payment_format') \\\n",
    "    .agg(\n",
    "        sum(col('is_laundering').cast('int')).alias('1'),\n",
    "        sum((1 - col('is_laundering')).cast('int')).alias('0')\n",
    "    ) \\\n",
    "    .withColumn(\"proportion\", (col(\"1\") / col(\"0\")).cast('Decimal(20,6)')) \\\n",
    "    .orderBy('proportion', ascending=False) \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "# Calculate the number of corresponding values for each value of the \"Payment Format\" and \"Is Laundering\" columns\n",
    "grouped_df = df.groupBy(\"payment_format\", \"is_laundering\").count()\n",
    "\n",
    "count_values = grouped_df.toPandas()\n",
    "count_values_payment = count_values.pivot(index='payment_format', columns='is_laundering', values='count')\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "bar_width = 0.35\n",
    "bar_positions = range(len(count_values_payment.index))\n",
    "\n",
    "#Arithmetic scale\n",
    "axs[0].bar(bar_positions, count_values_payment[0], bar_width, label='Is Laundering = 0')\n",
    "axs[0].bar([p + bar_width for p in bar_positions], count_values_payment[1], bar_width, label='Is Laundering = 1')\n",
    "axs[0].set_xticks(bar_positions)\n",
    "axs[0].set_xticklabels(count_values_payment.index, rotation='vertical') \n",
    "axs[0].set_xlabel('Payment Format')\n",
    "axs[0].set_ylabel('Number of corresponding values')\n",
    "axs[0].set_title('Bar chart in arithmetic scale')\n",
    "axs[0].legend()\n",
    "\n",
    "#Logaritmic scale\n",
    "axs[1].bar(bar_positions, count_values_payment[0], bar_width, label='Is Laundering = 0')\n",
    "axs[1].bar([p + bar_width for p in bar_positions], count_values_payment[1], bar_width, label='Is Laundering = 1')\n",
    "axs[1].set_xticks(bar_positions)\n",
    "axs[1].set_xticklabels(count_values_payment.index, rotation='vertical') \n",
    "axs[1].set_xlabel('Payment Format')\n",
    "axs[1].set_ylabel('Number of corresponding values')\n",
    "axs[1].set_title('Bar chart in logarithmic scale')\n",
    "axs[1].legend()\n",
    "axs[1].set_yscale('log')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'analisi sul formato di pagamento in relazione alle transazioni laundering mostra in maniera abbastanza evidente che la metodologia di pagamento ACH è molto spesso correlata ad una transazione fraudolenta, mentre, ancor più interessante, le metodologie Reinvestment e Wire non contengono alcun tipo di transazione fraudolenta. Questo fa intuire fin da subito che il formato di pagamento è una feature essenziale da tenere in considerazione, in quanto già da sola, se usata con un modello ad albero, dovrebbe essere in grado di classificare correttamente tutte le transazioni Reinvestment e Wire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display payment currency in relation to laundering transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('payment_currency', 'is_laundering') \\\n",
    "    .groupBy('payment_currency') \\\n",
    "    .agg(\n",
    "        sum(col('is_laundering').cast('int')).alias('1'),\n",
    "        sum((1 - col('is_laundering')).cast('int')).alias('0')\n",
    "    ) \\\n",
    "    .withColumn(\"proportion\", (col(\"1\") / col(\"0\")).cast('Decimal(20,6)')) \\\n",
    "    .orderBy('proportion', ascending=False) \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "grouped_df = df.groupBy(\"payment_currency\", \"is_laundering\").count()\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "count_values = grouped_df.toPandas()\n",
    "\n",
    "# Use the unstack() method\n",
    "count_values_currency = count_values.pivot(index='payment_currency', columns='is_laundering', values='count')\n",
    "\n",
    "# Sort the values by Is Laundering = 1 in descending order\n",
    "count_values_currency = count_values_currency.sort_values(1, ascending=False)\n",
    "\n",
    "# Create a bar chart with a logarithmic scale\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "bar_width = 0.35\n",
    "bar_positions = range(len(count_values_currency.index))\n",
    "axs[0].bar(bar_positions, count_values_currency[0], bar_width, label='Is Laundering = 0')\n",
    "axs[0].bar([p + bar_width for p in bar_positions], count_values_currency[1], bar_width, label='Is Laundering = 1')\n",
    "axs[0].set_xticks(bar_positions)\n",
    "axs[0].set_xticklabels(count_values_currency.index, rotation='vertical') \n",
    "axs[0].set_xticklabels(count_values_currency.index)\n",
    "axs[0].set_xlabel('Payment Currency')\n",
    "axs[0].set_ylabel('Number of corresponding values')\n",
    "axs[0].set_title('Bar chart in arithmetic scale')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].bar(bar_positions, count_values_currency[0], bar_width, label='Is Laundering = 0')\n",
    "axs[1].bar([p + bar_width for p in bar_positions], count_values_currency[1], bar_width, label='Is Laundering = 1')\n",
    "axs[1].set_xticks(bar_positions)\n",
    "axs[1].set_xticklabels(count_values_currency.index, rotation='vertical') \n",
    "axs[1].set_xticklabels(count_values_currency.index)\n",
    "axs[1].set_xlabel('Payment Currency')\n",
    "axs[1].set_ylabel('Number of corresponding values')\n",
    "axs[1].set_title('Bar chart in logarithmic scale')\n",
    "axs[1].legend()\n",
    "axs[1].set_yscale('log')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo studio sulle metodologie di pagamento in relazione alle transazioni fraudolente risulta essere meno incisivo di quello sul formato di pagamento. In ogni caso dimostra come la tipologia di pagamento US Dollar, che sembra essere anche la più frequente all'interno del dataset, contiene il maggior numero di transazioni fraudolente. Nonostante questo, a livello di proporzione, sembra essere molto più fraudolento il metodo di pagamento Saudi Riyal, seguito da Euro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display relationhip between amount paid and laundering transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_stats = df.groupBy('is_laundering').agg(\n",
    "    min(col('amount_paid')).alias('min').cast('Decimal(20,6)').alias('min'),\n",
    "    max(col('amount_paid')).alias('max').alias('max'),\n",
    "    mean(col('amount_paid')).alias('mean').alias('mean')\n",
    ")\n",
    "\n",
    "grouped_stats.show()\n",
    "\n",
    "# Sottocampiona la classe maggioritaria\n",
    "df_0 = df.filter(df['is_laundering'] == 0).sample(withReplacement=False, fraction=100000/5000000)\n",
    "df_1 = df.filter(df['is_laundering'] == 1)\n",
    "df_sampled = df_0.union(df_1)\n",
    "\n",
    "\n",
    "df_pd = df_sampled.toPandas()\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: format(x, ',.2f')))\n",
    "\n",
    "# Plotta i punti per is_laundering == 0 in blu e is_laundering == 1 in rosso\n",
    "plt.scatter(df_pd[df_pd['is_laundering'] == 0]['is_laundering'], df_pd[df_pd['is_laundering'] == 0]['amount_paid'], color='blue', alpha=0.5, label='Not Laundering')\n",
    "plt.scatter(df_pd[df_pd['is_laundering'] == 1]['is_laundering'], df_pd[df_pd['is_laundering'] == 1]['amount_paid'], color='red', alpha=0.5, label='Laundering')\n",
    "\n",
    "plt.title(\"Relationship between Is Laundering and Amount Paid\")\n",
    "plt.xlabel(\"Is Laundering\")\n",
    "plt.ylabel(\"Amount Paid\")\n",
    "plt.xticks([0, 1])\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A differenza delle precedenti analisi, andando a studiare il comportamento che hanno le transazioni fraudolente rispetto all'ammontare della somma inviata, si nota come non ci sia alcun tipo di separazione tra transazione fraudolenta o non fraudolenta. Infatti, utilizzando un grafico, è possibile notare come il range di amount delle transazioni fraudolenti è contenuto all'interno di quelle non fraudolenti senza un effettivo stacco. Oltretutto, considerando che le transazioni fraudolenti sono circa 1/1000 di quelle non fraudolenti, si può evincere che molto probabilmente l'amount della transazione sporca e non aiuta la capacità del modello a classificare correttamente i dati. \n",
    "\n",
    "Le due features verranno comunque mantenute per osservarle meglio nella matrice di correlazione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display top 10 accounts for fraudolent transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola il numero totale di transazioni per ogni account\n",
    "total_transactions = df.groupBy('from_account')\\\n",
    "    .agg(F.count('*').alias('total_trans'))\n",
    "\n",
    "# Conta il numero di transazioni fraudolente per ogni account\n",
    "fraudulent_transactions = df.filter(col('is_laundering') == 1)\\\n",
    "    .groupBy('from_account')\\\n",
    "    .agg(F.count('*').alias('count_laundering'))\n",
    "\n",
    "# Unisci i due conteggi\n",
    "joined_df = total_transactions.join(fraudulent_transactions, 'from_account', 'left_outer')\\\n",
    "    .fillna(0)  # In caso non ci siano transazioni fraudolente per un determinato account\n",
    "\n",
    "# Calcola il rapporto\n",
    "joined_df = joined_df.withColumn('fraud_rate', (F.col('count_laundering') / F.col('total_trans')).cast('Decimal(20,6)'))\n",
    "\n",
    "# Ordina e mostra i risultati\n",
    "joined_df.orderBy('total_trans', ascending=False).show(10)\n",
    "print(f\"Number of account that send just one transaction and it is fraudolent: {joined_df.filter((col('total_trans') == 1) & (col('fraud_rate') == 1)).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find how many times an account send laundering and not laundering transaction to same account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.select('from_account', 'to_account', 'is_laundering')\n",
    "\n",
    "# Raggruppa e conta le occorrenze uniche\n",
    "grouped_df = df_temp.groupBy('from_account', 'to_account').agg(collect_set('is_laundering').alias('unique_values'))\n",
    "\n",
    "# Filtra i risultati con più di una occorrenza\n",
    "filtered_df = grouped_df.filter(col('unique_values').getItem(0) != col('unique_values').getItem(1))\n",
    "\n",
    "# Calcola il numero di occorrenze filtrate per ogni 'from_account'\n",
    "result_df = filtered_df.groupBy('from_account').count().orderBy(col('count').desc())\n",
    "\n",
    "# Mostra i primi 10 risultati\n",
    "result_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per quanto riguarda gli account si evince come ci siano account che hanno effettuato nettamente più transazioni di altri. A parte questo, l'utilizzo degli accoun come features non è rilevante ai fini dell'allenamento del modello. Possiamo però andare a studiare il comportamento delle transazioni quando gli account sono uguali o diversi e capire se c'è una particolare correlazione con transazioni fraudolente. Stesso può essere fatto con la valuta di partenza e di arrivo e con l'amount di partenza e di arrivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show relationship between same account and fraudolent transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra le righe e raggruppa per is_laundering\n",
    "grouped_df = df.filter(F.col('from_account') == F.col('to_account'))\\\n",
    "    .groupBy('is_laundering')\\\n",
    "    .agg(F.count('*').alias('count'))\n",
    "\n",
    "count_total = df.groupBy(col('is_laundering').alias('is_laundering_temp')).agg(count('*').alias('count_total'))\n",
    "grouped_df = grouped_df.join(count_total, col('is_laundering') == col('is_laundering_temp')).drop('is_laundering_temp').withColumn('proportion', (col('count') / col('count_total')).cast('Decimal(20,6)')).drop('count_total')\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da queste analisi risulta molto poco probabile che una transazione che contenga lo stesso account di partenza e di destinazione sia fraudolenta. Questa potrebbe rilevarsi una features interessante da considerare e aggiungere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_same_account(df):\n",
    "    return df.withColumn('same_account', when((col('from_account') == col('to_account')), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiunta della feature che considera se l'account di partenza e destinazione sono uguali\n",
    "df = add_same_account(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show relationship between same bank and fraudolent transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra le righe e raggruppa per is_laundering\n",
    "grouped_df = df.filter(F.col('from_bank') == F.col('to_bank'))\\\n",
    "    .groupBy('is_laundering')\\\n",
    "    .agg(F.count('*').alias('count'))\n",
    "\n",
    "count_total = df.groupBy(col('is_laundering').alias('is_laundering_temp')).agg(count('*').alias('count_total'))\n",
    "grouped_df = grouped_df.join(count_total, col('is_laundering') == col('is_laundering_temp')).drop('is_laundering_temp').withColumn('proportion', (col('count') / col('count_total')).cast('Decimal(20,6)')).drop('count_total')\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_df = df.select(col(\"from_account\").alias(\"account\"), col(\"from_bank\").alias(\"bank\"))\n",
    "to_df = df.select(col(\"to_account\").alias(\"account\"), col(\"to_bank\").alias(\"bank\"))\n",
    "combined_df = from_df.union(to_df)\n",
    "\n",
    "# Raggruppa per account e conta le banche distinte\n",
    "result_df = combined_df.groupBy(\"account\").agg(countDistinct(\"bank\").alias(\"num_banks\"))\n",
    "\n",
    "# Filtra gli account con più di una banca associata\n",
    "filtered_df = result_df.filter(result_df[\"num_banks\"] > 1)\n",
    "\n",
    "show_df = filtered_df.orderBy('num_banks', ascending=False)\n",
    "print(f'Number of account with more than one bank associated: {show_df.count()}')\n",
    "# 4. Mostra i risultati\n",
    "show_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo studio dimostra che ci sono 8 account che hanno più di una banca associata e che il massimo di banche associate è 2. Inoltre dimostra che c'è una più forte correlazione tra la banca di invio e ricezione uguale rispetto allo stesso account. Ovviamente una stessa banca può appartenere a più utenti. E' anche questa una feature che potrebbe tornare utile nella classificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_same_bank(df):\n",
    "    return df.withColumn('same_bank', when((col('from_bank') == col('to_bank')), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiunta della feature che considera se l'account di partenza e destinazione sono uguali\n",
    "df = add_same_bank(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show relationship between same amount and fraudolent transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra le righe e raggruppa per is_laundering\n",
    "grouped_df = df.filter(F.col('amount_received') == F.col('amount_paid'))\\\n",
    "    .groupBy('is_laundering')\\\n",
    "    .agg(F.count('*').alias('count'))\n",
    "\n",
    "count_total = df.groupBy(col('is_laundering').alias('is_laundering_temp')).agg(count('*').alias('count_total'))\n",
    "grouped_df = grouped_df.join(count_total, col('is_laundering') == col('is_laundering_temp')).drop('is_laundering_temp').withColumn('proportion', (col('count') / col('count_total')).cast('Decimal(20,6)')).drop('count_total')\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso possiamo vedere che tutte le transazioni che sono fraudolente hanno l'amount inviato e ricevuto uguale. Di conseguenza possiamo dedurre che abbiano anche la stessa valuta di invio e ricezione, in quanto sennò ci sarebbe una differenza dovuta al cambio valuta. In ogni caso, seppur questa possa sembrare una feature interessante, mettendola in relazione anche con le transazioni non fraudolente, risulta che non c'è molta differenza. In poche parole, qualora amount received e send non fossero uguali, la classificazione sarebbe corretta. C'è però da considerare che solo l'1,5% del dataset presenta questa caratteristica, dunque potrebbe non essere così impattante. \n",
    "\n",
    "Effettuiamo comunque un'analisi per vedere anche se ci sono casi in cui l'amount è diverso nonostante le currencies siano uguali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show relationship between same currency and fraudolent transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra le righe e raggruppa per is_laundering\n",
    "grouped_df = df.filter(F.col('receiving_currency') == F.col('payment_currency'))\\\n",
    "    .groupBy('is_laundering')\\\n",
    "    .agg(F.count('*').alias('count'))\n",
    "\n",
    "count_total = df.groupBy(col('is_laundering').alias('is_laundering_temp')).agg(count('*').alias('count_total'))\n",
    "grouped_df = grouped_df.join(count_total, col('is_laundering') == col('is_laundering_temp')).drop('is_laundering_temp').withColumn('proportion', (col('count') / col('count_total')).cast('Decimal(20,6)')).drop('count_total')\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questa analisi dimostra che non c'è una correlazione 1:1 tra amount e currency in quanto si evince che ci siano transazioni che non hanno la stessa valuta di invio e ricezione ma hanno lo stesso amount. La differenza è però trascurabile. In ogni caso, considerando che anche questo studio ha portato a dimostrare che tutte le transazioni fraudolente hanno la stessa valuta, ma meno transazioni non fraudolente hanno la stessa valuta, andiamo a considerare solo 'same_amount' come feature da aggiungere in quanto, seppur di poco, ci offre uno stacco maggiore di proporzione rispetto ai laundering.\n",
    "\n",
    "Rimane comunque che questa feature non porta ad una separazione delle classi considerevole e dunque in un processo di feature selection potrebbe venir scartata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_same_currency(df):\n",
    "    return df.withColumn('same_currency', when((col('receiving_currency') == col('payment_currency')), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiunta della feature che considera se la valuta di partenza e destinazione sono uguali\n",
    "df = add_same_currency(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi considerando il timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come prima cosa è necessario andare a fare un casting del timestamp da stringa a DateType. Oltre a questo vado a separare il timestamp in componenti separate così da poterlo analizzare più facilmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"timestamp\", to_timestamp(\"timestamp\", \"yyyy/MM/dd HH:mm\"))\n",
    "\n",
    "# Split the timestamp column into separate components\n",
    "df = df.withColumn(\"year\", year(\"timestamp\"))\\\n",
    "                             .withColumn(\"month\", month(\"timestamp\"))\\\n",
    "                             .withColumn(\"day\", dayofmonth(\"timestamp\"))\\\n",
    "                             .withColumn(\"hour\", hour(\"timestamp\"))\\\n",
    "                             .withColumn(\"minute\", minute(\"timestamp\"))\n",
    "\n",
    "\n",
    "# Dato che ho fatto delle aggiunte di features, vado a fare il caching del dataset per migliorare le performance                \n",
    "df.cache()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show laundering per componenti del timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laundering_for(df, col_name: str):\n",
    "    print(f\"Laundering for {col_name}\")\n",
    "    df.select(col_name, 'is_laundering') \\\n",
    "    .groupBy(col_name) \\\n",
    "    .agg(\n",
    "        sum(col('is_laundering').cast('int')).alias('count(1)'),\n",
    "        sum((1 - col('is_laundering')).cast('int')).alias('count(0)'),\n",
    "    ).withColumn(\"ratio\", (col('count(1)')/col('count(0)')).cast('Decimal(20,6)')) \\\n",
    "  .orderBy(col('ratio').desc()) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laundering per Anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laundering_for(df, 'year')\n",
    "\n",
    "# Remove year feature\n",
    "df = df.drop('year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dataset presenta un solo anno di transazioni, quindi come feature 'year' risulta inutile. Per questo motivo andiamo a rimuoverla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laundering per Mese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laundering_for(df, 'month')\n",
    "\n",
    "# Remove year feature\n",
    "df = df.drop('month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come per l'anno, anche 'month' ha un solo valore per tutte le righe. Procediamo dunque a rimuoverla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laundering per Giorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laundering_for(df, 'day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laundering per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laundering_for(df, 'hour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laundering for Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laundering_for(df, 'minute')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A differenza del mese e dell'anno, i giorni, le ore e i minuti possono essere considerati come feature importanti. Si nota infatti come determinati valori sono più propensi ad essere fraudolenti rispetto ad altri. Andremo ad utilizzare queste tre nuove features per calcolarne di nuove in relazione agli account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of transactions an account receive in different period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trans_received(df):\n",
    "    window = Window.partitionBy('to_account', 'day')\n",
    "    df = df.withColumn(\"transaction_received_per_day\", count('*').over(window))\n",
    "\n",
    "    window = Window.partitionBy('to_account', 'hour')\n",
    "    df = df.withColumn(\"transaction_received_per_hour\", count('*').over(window))\n",
    "\n",
    "    window = Window.partitionBy('to_account', 'week')\n",
    "    df = df.withColumn(\"transaction_received_per_week\", count('*').over(window))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_trans_received(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of transactions an account send in different period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trans_send(df):\n",
    "    window = Window.partitionBy('from_account', 'day')\n",
    "    df = df.withColumn(\"transaction_send_per_day\", count('*').over(window))\n",
    "\n",
    "    window = Window.partitionBy('from_account', 'hour')\n",
    "    df = df.withColumn(\"transaction_send_per_hour\", count('*').over(window))\n",
    "\n",
    "    window = Window.partitionBy('from_account', 'week')\n",
    "    df = df.withColumn(\"transaction_send_per_week\", count('*').over(window))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_trans_send(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the minutes since last transaction send by an Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_minutes_since_last_trans(df):\n",
    "\n",
    "    windowSpec = Window.partitionBy(\"from_account\").orderBy(\"timestamp\")\n",
    "\n",
    "    # Usa la funzione 'lag' per ottenere il timestamp della transazione precedente\n",
    "    df = df.withColumn(\"prev_timestamp\", F.lag(df.timestamp).over(windowSpec))\n",
    "\n",
    "    # Calcola la differenza in minuti, -1 se è la prima transazione che effettua\n",
    "    df = df.withColumn(\n",
    "        \"minutes_since_last_transaction\", \n",
    "        F.when(\n",
    "            F.isnull(df.prev_timestamp), \n",
    "            -1\n",
    "        ).otherwise(\n",
    "            (F.unix_timestamp(df.timestamp) - F.unix_timestamp(df.prev_timestamp)) / 60\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Seleziona le colonne desiderate\n",
    "    return df.drop(\"prev_timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_minutes_since_last_trans(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count how many transactions are sent to unique accounts and bank in different period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unique_to_bank_account(df):\n",
    "    df = df.withColumn(\"timestamp_minutes\", F.unix_timestamp(\"timestamp\")/60)\n",
    "\n",
    "\n",
    "    #Day\n",
    "    windowSpec = Window.partitionBy(\"from_account\").orderBy(\"timestamp_minutes\")\\\n",
    "                .rangeBetween(-1440, Window.currentRow)\n",
    "    df = df.withColumn(\"unique_to_banks_last_day\", F.size(F.collect_set(\"to_bank\").over(windowSpec)))\n",
    "    df = df.withColumn(\"unique_to_accounts_last_day\", F.size(F.collect_set(\"to_account\").over(windowSpec)))\n",
    "\n",
    "\n",
    "    return df.drop('timestamp_minutes')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_unique_to_bank_account(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count how many transactions are sent to unique accounts with same payment format in different period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unique_payment_format(df):\n",
    "    df = df.withColumn(\"timestamp_minutes\", F.unix_timestamp(\"timestamp\")/60)\n",
    "    \n",
    "    #Day\n",
    "    windowSpec = Window.partitionBy(\"from_account\").orderBy(\"timestamp_minutes\")\\\n",
    "                .rangeBetween(-1440, Window.currentRow)\n",
    "    df = df.withColumn(\"unique_payment_formats_last_day\", F.size(F.collect_set(\"payment_format\").over(windowSpec)))\n",
    "\n",
    "    return df.drop('timestamp_minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_unique_payment_format(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count how many transactions are sent to unique accounts with same payment currency in different period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unique_payment_currency(df):\n",
    "    df = df.withColumn(\"timestamp_minutes\", F.unix_timestamp(\"timestamp\")/60)\n",
    "\n",
    "    #Day\n",
    "    windowSpec = Window.partitionBy(\"from_account\").orderBy(\"timestamp_minutes\")\\\n",
    "                .rangeBetween(-1440, Window.currentRow)\n",
    "    df = df.withColumn(\"unique_payment_currency_last_day\", F.size(F.collect_set(\"payment_currency\").over(windowSpec)))\n",
    "\n",
    "    return df.drop('timestamp_minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_unique_payment_currency(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count how many transactions an account execute to the same other account until the transaction itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transaction_recurrence(df):\n",
    "    windowSpec = Window.partitionBy(\"from_account\", \"to_account\").orderBy(\"timestamp\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    return df.withColumn(\"transaction_recurrence\", F.count(\"timestamp\").over(windowSpec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_transaction_recurrence(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataframe in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(32).write.parquet('partial_results/df.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il label encoding viene fatto su tutte le features non numeriche così da poter successivamente creare la matrice di correlazione e utilizzare le features anche per allenare e testare il modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mapping(df, col1, col2=None):\n",
    "    unique_columns = set(df.select(col1).orderBy(col1).distinct().rdd.flatMap(lambda x: x).collect())\n",
    "    if col2:\n",
    "        unique_columns.update(df.select(col2).distinct().rdd.flatMap(lambda x: x).collect())\n",
    "    return {column: index for index, column in enumerate(unique_columns)}\n",
    "\n",
    "def label_columns(df, col1, mapping, col2=None):\n",
    "    column_to_index_udf = udf(lambda column: mapping[column], IntegerType())\n",
    "    \n",
    "    df_to_return = df.withColumn(f\"{col1}_indexed\", column_to_index_udf(col(col1))).drop(col1).withColumnRenamed(f'{col1}_indexed', col1) \n",
    "    if col2:\n",
    "        df_to_return = df_to_return.withColumn(f\"{col2}_indexed\", column_to_index_udf(col(col2))).drop(col2).withColumnRenamed(f'{col2}_indexed', col2)\n",
    "    return df_to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display correlation matrix\n",
    "\n",
    "A correlation matrix is a table showing the correlation coefficients between variables. Each cell in the table shows the correlation between two variables. The value is in the range of -1 to 1. If two variables have high correlation, it means when one variable changes, the second tends to change in a specific direction. A value close to 1 implies a strong positive correlation: as one variable increases, the other also tends to increase. A value close to -1 implies a strong negative correlation: as one variable increases, the other tends to decrease. A value close to 0 implies a weak or no linear correlation between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def encode_columns(df, col1, col2=None):\n",
    "    le = LabelEncoder()\n",
    "    if col2:\n",
    "        le.fit(list(set(df[col1]).union(set(df[col2]))))\n",
    "        df[col1] = le.transform(df[col1])\n",
    "        df[col2] = le.transform(df[col2])\n",
    "    else:\n",
    "        df[col1] = le.fit_transform(df[col1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('partial_results/df.parquet')\n",
    "\n",
    "df = encode_columns(df, 'payment_format')\n",
    "df = encode_columns(df, 'from_account', 'to_account')\n",
    "df = encode_columns(df, 'receiving_currency', 'payment_currency')\n",
    "\n",
    "df = df.drop('timestamp', axis=1)\n",
    "\n",
    "laundering_corr_matrix = df.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(22, 8))\n",
    "sns.heatmap(laundering_corr_matrix, cmap='inferno', annot=False, ax=ax)\n",
    "ax.set_title('Correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice di correlazione mostra come non ci siano forti correlazioni tra le etichette e le features prese in esame. Viceversa si può notare come ci siano features strettamente correlate tra di loro che potrebbero causare ridondanza nei dati oltre che aumentare il numero di features rendendo il modello più complesso e causare overfitting. Le forti correlazioni vengono però risolte in quanto viene utilizzato un modello di apprendimento random forest che va a campionare e selezionare diverse features per ogni albero.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Organization and Pre-processing\n",
    "\n",
    "In questa sezione andremo a dividere il dataset per effettuare il training e il test dei modelli. La divisione scelta è 60% per il Train set, il 20% per il validation set (Hyperparameter tuning) e il 20% per il Test set. La divisione è stata suggerita dalla descrizione presente sul sito Kaggle riguardante questo dataset.\n",
    "\n",
    "Una volta diviso il dataset verrà eseguito il preprocessing dei dati, dunque verranno tolte le features che non sono utili ai fini della predizione e infine verranno salvati i diversi set di dati per essere utilizzati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting\n",
    "\n",
    "Come detto precedentemente, i dati vengono divisi in 60% train, 20% validation e 20% test\n",
    "\n",
    "Prima di effettuare la divisione i dati vengono ordinati per timestamp, per poi suddividerli in maniera sequenziale. Di norma questa procedura non andrebbe fatta, ma viene effettuata in quanto la logica delle transazioni si basa sul tempo e ci sono molte features collegate ad esso. Se i dati fossero presi in maniera randomica, le features temporali assumerebbero molta meno importanza.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('partial_results/df.parquet')\n",
    "ordered_df = df.orderBy(\"timestamp\")\n",
    "\n",
    "mapping_format = create_mapping(df, 'payment_format')\n",
    "mapping_currency = create_mapping(df, 'payment_currency', 'receiving_currency')\n",
    "\n",
    "ordered_df.cache()\n",
    "# Calculate row counts for splits\n",
    "total_rows = ordered_df.count()\n",
    "train_rows, validation_rows = int(total_rows * 0.6), int(total_rows * 0.2)\n",
    "test_rows = total_rows - train_rows - validation_rows\n",
    "\n",
    "# Add a dummy partition and assign row numbers based on ordered timestamps\n",
    "w = Window.partitionBy(lit(1)).orderBy(\"timestamp\")\n",
    "ordered_df = ordered_df.withColumn(\"row_number\", F.row_number().over(w))\n",
    "\n",
    "# Split and repartition the DataFrame into train, validation, and test sets based on row numbers\n",
    "train_df = ordered_df.filter(col(\"row_number\") <= train_rows).drop(\"row_number\", \"dummy_partition\").repartition(32)\n",
    "validation_df = ordered_df.filter(col(\"row_number\").between(train_rows + 1, train_rows + validation_rows)).drop(\"row_number\", \"dummy_partition\").repartition(32)\n",
    "test_df = ordered_df.filter(col(\"row_number\") > train_rows + validation_rows).drop(\"row_number\", \"dummy_partition\").repartition(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate features over splitted data and save\n",
    "\n",
    "In questa sezione vado ad effettuare il pre-processing sui dati. Vengono calcolate dunque tutte le features necessarie che sono state utilizzate per l'analisi e vengono rimosse le features che non si reputa possano essere importanti.\n",
    "\n",
    "In questo caso le features rimosse sono la banca di provenienza ed arrivo in quanto ci si aspetta che siano valori di poco conto in un sistema in cui magari si allena il modello su determinati tipi di banche e magari lo si usa per predirre dati in cui non compaiono le stesse. Il ragionamento è il medesimo per l'account di provenienza e arrivo. E' stato deciso inoltre di andare a togliere gli amount in quanto già dalle analisi si dimostravano poco utili e molto randomici.\n",
    "\n",
    "Index e timestamp vengono tolti in quanto inutili per l'addestramento e l'utilizzo del modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary_features(df):\n",
    "    df = df.drop('from_bank')\n",
    "    df = df.drop('to_bank')\n",
    "    df = df.drop('from_account')\n",
    "    df = df.drop('to_account')\n",
    "    df = df.drop('amount_received')\n",
    "    df = df.drop('amount_paid')\n",
    "    df = df.drop('index')\n",
    "    df = df.drop('timestamp')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dati vengono salvati in formato parquet così da poterli riutilizzare senza ricalcolare ogni volta tutte le features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df, name):\n",
    "    df = label_columns(df, 'receiving_currency', mapping_currency, 'payment_currency')\n",
    "    df = label_columns(df,'payment_format', mapping_format)\n",
    "\n",
    "    df = add_same_account(df)\n",
    "    df = add_same_bank(df)\n",
    "    df = add_same_currency(df)\n",
    "    df = add_trans_received(df)\n",
    "    df = add_trans_send(df)\n",
    "    df = add_minutes_since_last_trans(df)\n",
    "    df = add_unique_to_bank_account(df)\n",
    "    df = add_unique_payment_format(df)\n",
    "    df = add_unique_payment_currency(df)\n",
    "    df = add_transaction_recurrence(df)\n",
    "    df = remove_unnecessary_features(df)\n",
    "    df.coalesce(32).write.parquet(f'partial_results/df.{name}', mode='overwrite')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocess_df(train_df, 'train')\n",
    "test_df = preprocess_df(test_df, 'test')\n",
    "validation_df = preprocess_df(validation_df, 'val')\n",
    "ordered_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "I dataset precedentemente salvati contengono un elevato numero di features, ma non per forza tutte le feature sono utili per un buon modello predittivo. Per questo motivo ho deciso di utilizzare una tecnica di feature selection denominata Boruta.\n",
    "\n",
    "Boruta è un algoritmo di selezione delle features per modelli basati su random forest. Esso identifica e conserva le caratteristiche più importanti per la previsione, eliminando quelle irrilevanti o ridondanti. Boruta agisce confrontando l'importanza delle caratteristiche originali con quella di caratteristiche casuali \"ombra\" (shadow features). Se una caratteristica originale è meno importante di una caratteristica ombra, viene considerata irrilevante e rimossa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "train_pd = pd.read_parquet('./partial_results/df.train')\n",
    "\n",
    "X_train = train_pd.drop('is_laundering', axis=1)\n",
    "y_train = train_pd['is_laundering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced_subsample', n_estimators=30)\n",
    "\n",
    "# Definisci Boruta\n",
    "boruta_selector = BorutaPy(rf, n_estimators='auto', random_state=0)\n",
    "\n",
    "# Addestra Boruta\n",
    "boruta_selector.fit(np.array(X_train.values), np.array(y_train.values.ravel()))\n",
    "\n",
    "# Seleziona le caratteristiche rilevanti\n",
    "X_train_selected = boruta_selector.transform(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = X_train.columns[boruta_selector.support_]\n",
    "removed_features = X_train.columns[[True if x == False else False for x in boruta_selector.support_]]\n",
    "print(\"Selected Features:\", selected_features)\n",
    "print(\"Removed Features:\", removed_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boruta ha selezionato quattro features che reputa non utili, dunque andrò a toglierle dai set di dati. Da notare come tra le features non selezionate vi è 'same_currency', che già dalle analisi preliminari risultava poco significativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_boruta_selected_features(df, name):\n",
    "    df = df.drop('same_currency')\n",
    "    df = df.drop('week')\n",
    "    df = df.drop('receiving_currency')\n",
    "    df = df.drop('payment_currency')\n",
    "\n",
    "    df.coalesce(32).write.parquet(f'partial_results/df.{name}', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_boruta_selected_features(train_df, 'train')\n",
    "remove_boruta_selected_features(test_df, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "A questo punto è possibile andare ad implementare l'albero decisionale from scratch. Un Decision Tree Classifier è un algoritmo di apprendimento supervisionato che utilizza una struttura ad albero per prendere decisioni. Ogni nodo dell'albero rappresenta una domanda o una condizione sui dati, mentre ogni ramo rappresenta un possibile risultato di quella domanda. Le foglie dell'albero rappresentano le classificazioni finali dei dati. L'algoritmo apprende come suddividere i dati basandosi sulle caratteristiche (features) per produrre le decisioni/classificazioni più accurate possibili.\n",
    "\n",
    " L'albero verrà testato utilizzando inizialmente dei dati sample presi dalla librearia di scikit-learn. Per questo scopo ho deciso di utilizzare il dataset Breast Tumor in quanto presenta una classificazione binaria e un numero elevato di features, così da poter testare l'efficacia dell'albero. Verrà poi messo a confronto con il decision tree classifier disponibile direttamente nella libreria di scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementazione dell'albero from scratch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTC():\n",
    "    def __init__(self, random_state = None, max_depth = None, min_sample_split = 2, criterion = \"gini\", min_info_gain = 0, max_features = None, max_thresholds = None, class_weights = {}, verbose=False):\n",
    "        \"\"\"\n",
    "        Initializes the Decision Tree Classifier.\n",
    "\n",
    "        Parameters:\n",
    "        - random_state: int or None, optional (default=None)\n",
    "            Seed for the random number generator.\n",
    "        - max_depth: int or None, optional (default=None)\n",
    "            The maximum depth of the decision tree. If None, the tree is grown until all leaves are pure or until all leaves contain less than min_sample_split samples.\n",
    "        - min_sample_split: int, optional (default=2)\n",
    "            The minimum number of samples required to split an internal node.\n",
    "        - criterion: str, optional (default=\"gini\")\n",
    "            The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity, \"entropy\" for the information gain, and \"shannon\" for the Shannon entropy.\n",
    "        - min_info_gain: float, optional (default=0)\n",
    "            The minimum information gain required to split an internal node.\n",
    "        - max_features: int, float, \"sqrt\", \"log2\", or None, optional (default=None)\n",
    "            The number of features to consider when looking for the best split. If int, then consider max_features features at each split. If float, then max_features is a fraction and int(max_features * n_features) features are considered. If \"sqrt\", then max_features=sqrt(n_features). If \"log2\", then max_features=log2(n_features). If None, then all features are considered.\n",
    "        - max_thresholds: int or None, optional (default=None)\n",
    "            The maximum number of thresholds to consider for each feature when looking for the best split. If None, all unique feature values are considered as potential thresholds.\n",
    "        - class_weights: dict, optional (default={})\n",
    "            Weights associated with classes. If provided, the class probabilities are multiplied by the corresponding weight.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.criterion = criterion\n",
    "        self.min_info_gain = min_info_gain\n",
    "        self.max_features = max_features\n",
    "        self.max_thresholds = max_thresholds\n",
    "        self.class_weights = class_weights\n",
    "        self.tree = None\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(self.random_state)\n",
    "        self.node_counter = 0\n",
    "        self.verbose = verbose\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the decision tree classifier to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: array-like of shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        - y: array-like of shape (n_samples,)\n",
    "            The target values.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.node_counter = 0\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = np.array(X)\n",
    "        if isinstance(y, pd.DataFrame):\n",
    "            y = np.array(y)\n",
    "\n",
    "        self.tree = self.__create_tree(X, y)\n",
    "       \n",
    "    def __calculate_split_entropy(self, y):\n",
    "        \"\"\"\n",
    "        Calculates the split entropy for a given target variable.\n",
    "\n",
    "        Parameters:\n",
    "        - y: array-like of shape (n_samples,)\n",
    "            The target values.\n",
    "\n",
    "        Returns:\n",
    "        split_criterion: float\n",
    "            The split criterion value.\n",
    "        \"\"\"\n",
    "\n",
    "        unique_values, value_counts = np.unique(y, return_counts=True)\n",
    "        class_probabilities = value_counts / len(y)\n",
    "\n",
    "        if len(self.class_weights) > 0 and len(unique_values) > 0:\n",
    "            class_probabilities *= np.array([self.class_weights.get(value, 1.0) for value in unique_values])\n",
    "\n",
    "        split_criteria = {\n",
    "            'shannon': lambda probs: -np.sum(probs * np.log2(probs)),\n",
    "            'entropy': lambda probs: -np.sum(probs * np.log2(probs + 1e-10)),\n",
    "            'gini': lambda probs:  1 - np.sum(probs ** 2)\n",
    "        }\n",
    "\n",
    "        split_criterion_function = split_criteria.get(self.criterion, split_criteria['gini'])\n",
    "        split_criterion = split_criterion_function(class_probabilities)\n",
    "\n",
    "        return split_criterion\n",
    "\n",
    "\n",
    "    def __calculate_info_gain(self, X, y, feature, threshold):\n",
    "        \"\"\"\n",
    "        Calculates the information gain for a given feature and threshold.\n",
    "\n",
    "        Parameters:\n",
    "        - X: array-like of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        - y: array-like of shape (n_samples,)\n",
    "            The target values.\n",
    "        - feature: int\n",
    "            The index of the feature.\n",
    "        - threshold: float\n",
    "            The threshold value.\n",
    "\n",
    "        Returns:\n",
    "        info_gain: float\n",
    "            The information gain.\n",
    "        \"\"\"\n",
    "\n",
    "        left_indeces = X[:, feature] <= threshold\n",
    "        right_indeces = X[:, feature] > threshold\n",
    "\n",
    "        left_labels = y[left_indeces]\n",
    "        right_labels = y[right_indeces]\n",
    "\n",
    "        left_side_entropy = self.__calculate_split_entropy(left_labels)\n",
    "        right_side_entropy = self.__calculate_split_entropy(right_labels)\n",
    "\n",
    "        \n",
    "        weighted_left_side_entropy = (len(left_labels) / len(y)) * left_side_entropy\n",
    "        weighted_right_side_entropy = (len(right_labels) / len(y)) * right_side_entropy\n",
    "\n",
    "        parent_entropy = self.__calculate_split_entropy(y)\n",
    "\n",
    "        info_gain = parent_entropy - (weighted_left_side_entropy + weighted_right_side_entropy)\n",
    "\n",
    "        return info_gain\n",
    "\n",
    "\n",
    "    def __get_features(self, n_features):\n",
    "        \"\"\"\n",
    "        Returns the indices of the features to consider for splitting.\n",
    "\n",
    "        Parameters:\n",
    "        - n_features: int\n",
    "            The total number of features.\n",
    "\n",
    "        Returns:\n",
    "        columns_id: list\n",
    "            The indices of the features to consider.\n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(self.random_state + self.node_counter if self.random_state is not None else None)\n",
    "        if self.max_features is not None:\n",
    "            if self.max_features == \"sqrt\":\n",
    "                columns_id = np.random.choice(range(n_features), int(math.sqrt(n_features)), replace=False)\n",
    "            elif self.max_features == \"log2\":\n",
    "                columns_id = np.random.choice(range(n_features), int(math.log2(n_features)), replace=False)\n",
    "            elif isinstance(self.max_features, int):\n",
    "                if self.max_features > n_features:\n",
    "                    raise ValueError(\"Max features > number of features\")\n",
    "                elif self.max_features <= 0:\n",
    "                    raise ValueError(\"Max features must be > 0\")\n",
    "                columns_id = np.random.choice(range(n_features), self.max_features, replace=False)\n",
    "            elif isinstance(self.max_features, float):\n",
    "                if self.max_features > 1:\n",
    "                    raise ValueError(\"Max features > number of features\")\n",
    "                elif self.max_features <= 0:\n",
    "                    raise ValueError(\"Max features must be > 0\")\n",
    "                columns_id = np.random.choice(range(n_features), int(n_features * self.max_features), replace=False)\n",
    "        else:\n",
    "            columns_id = list(range(n_features))    \n",
    "\n",
    "        return columns_id\n",
    "    \n",
    "\n",
    "    def __get_thresholds(self, X, feature):\n",
    "        \"\"\"\n",
    "        Returns the thresholds to consider for a given feature.\n",
    "\n",
    "        Parameters:\n",
    "        - X: array-like of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        - feature: int\n",
    "            The index of the feature.\n",
    "\n",
    "        Returns:\n",
    "        thresholds: array-like\n",
    "            The thresholds to consider.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.random_state + self.node_counter if self.random_state is not None else None)\n",
    "        if self.max_thresholds is not None:\n",
    "            if self.max_thresholds <= 0:\n",
    "                raise ValueError(\"max_thresholds must be > 0\")\n",
    "            thresholds = np.percentile(X[:, feature], np.linspace(0, 100, self.max_thresholds))\n",
    "        else:\n",
    "            unique_vals = np.unique(X[:, feature])\n",
    "            thresholds = (unique_vals[1:] + unique_vals[:-1]) / 2\n",
    "        return thresholds\n",
    "\n",
    "\n",
    "\n",
    "    def __calculate_best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculates the best split for the given input samples and target values.\n",
    "\n",
    "        Parameters:\n",
    "        - X: array-like of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        - y: array-like of shape (n_samples,)\n",
    "            The target values.\n",
    "\n",
    "        Returns:\n",
    "        best_feature: int\n",
    "            The index of the best feature to split on.\n",
    "        best_threshold: float\n",
    "            The best threshold value.\n",
    "        best_info_gain: float\n",
    "            The information gain of the best split.\n",
    "        \"\"\"\n",
    "\n",
    "        best_threshold = None\n",
    "        best_info_gain = -np.inf\n",
    "        best_feature = None\n",
    "        self.node_counter += 1\n",
    "        features = self.__get_features(X.shape[1])\n",
    "\n",
    "        for feature in features:\n",
    "            threholds = self.__get_thresholds(X, feature)\n",
    "               \n",
    "            for threshold in threholds:\n",
    "                info_gain = self.__calculate_info_gain(X, y, feature, threshold)\n",
    "                if best_info_gain < info_gain:\n",
    "                    best_threshold = threshold\n",
    "                    best_feature = feature\n",
    "                    best_info_gain = info_gain\n",
    "\n",
    "        return best_feature, best_threshold, best_info_gain\n",
    "\n",
    "\n",
    "    def __create_tree(self, X, y, depth = 0):\n",
    "        \"\"\"\n",
    "        Recursively creates the decision tree.\n",
    "\n",
    "        Parameters:\n",
    "        - X: array-like of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        - y: array-like of shape (n_samples,)\n",
    "            The target values.\n",
    "        - depth: int, optional (default=0)\n",
    "            The current depth of the tree.\n",
    "\n",
    "        Returns:\n",
    "        tree: dict\n",
    "            The decision tree.\n",
    "        \"\"\"\n",
    "        samples = X.shape[0]\n",
    "        \n",
    "        if samples < self.min_sample_split or (self.max_depth != None and depth >= self.max_depth):\n",
    "            return self.__create_leaf_node(y)\n",
    "\n",
    "        best_feature, best_threshold, best_info_gain = self.__calculate_best_split(X, y)\n",
    "\n",
    "        if(best_info_gain <= self.min_info_gain):\n",
    "            return self.__create_leaf_node(y)\n",
    "        \n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        left_child = self.__create_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_child = self.__create_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        if 'label' in left_child and 'label' in right_child:\n",
    "            if left_child['label'] == right_child['label']:\n",
    "                return {\n",
    "                    'label': left_child['label'],\n",
    "                    'samples': len(y)\n",
    "                }\n",
    "\n",
    "\n",
    "        return {\n",
    "            'splitting_threshold': best_threshold,\n",
    "            'splitting_feature': best_feature,\n",
    "            'info_gain': best_info_gain,\n",
    "            'left_child': left_child,\n",
    "            'right_child': right_child\n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "    def __create_leaf_node(self, y):\n",
    "        \"\"\"\n",
    "        Creates a leaf node for the decision tree.\n",
    "\n",
    "        Parameters:\n",
    "        - y: array-like of shape (n_samples,)\n",
    "            The target values.\n",
    "\n",
    "        Returns:\n",
    "        leaf_node: dict\n",
    "            The leaf node.\n",
    "        \"\"\"\n",
    "        majority_class = Counter(y).most_common(1)[0][0]\n",
    "        return {\n",
    "            'label': majority_class,\n",
    "            'samples': len(y)\n",
    "        }\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for the input samples.\n",
    "\n",
    "        Parameters:\n",
    "        - X: array-like of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "\n",
    "        Returns:\n",
    "        predictions: array-like of shape (n_samples,)\n",
    "            The predicted class labels.\n",
    "        \"\"\"\n",
    "\n",
    "        if(isinstance(X, pd.DataFrame)):\n",
    "            X = np.array(X)\n",
    "        \n",
    "        predictions = []\n",
    "        for sample in X:\n",
    "            prediction = self.__traverse_tree(sample, self.tree)\n",
    "            predictions.append(prediction)\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def __calculate_metrics(self, y_true, y_pred):\n",
    "        # Inizializza le variabili\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        \n",
    "        # Conta TP, TN, FP, FN\n",
    "        for true, pred in zip(y_true, y_pred):\n",
    "            if true == 1 and pred == 1:\n",
    "                TP += 1\n",
    "            elif true == 0 and pred == 0:\n",
    "                TN += 1\n",
    "            elif true == 1 and pred == 0:\n",
    "                FN += 1\n",
    "            elif true == 0 and pred == 1:\n",
    "                FP += 1\n",
    "        \n",
    "        # Calcola precisione, recall, f1-score e accuracy\n",
    "        precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "        \n",
    "        return precision, recall, f1_score, accuracy\n",
    "\n",
    "    def score(self, X, y):\n",
    "        if(isinstance(y, pd.DataFrame)):\n",
    "            y = np.array(y)\n",
    "\n",
    "\n",
    "        y_pred = self.predict(X)\n",
    "        if(self.verbose):\n",
    "            precision, recall, f1_score, accuracy = self.__calculate_metrics(y, y_pred)\n",
    "            metrics = {\n",
    "                'accuracy': accuracy,\n",
    "                'recall': recall,\n",
    "                'precision': precision,\n",
    "                'f1_score': f1_score\n",
    "            }\n",
    "\n",
    "            print(metrics)\n",
    "            print(f\"max_depth: {self.max_depth}, min_sample_split: {self.min_sample_split}, criterion: {self.criterion}, max_features: {self.max_features}, max_thresholds: {self.max_thresholds}\")\n",
    "            \n",
    "        return np.mean(y_pred == y)\n",
    "    \n",
    "\n",
    "    def __traverse_tree(self, sample, node):\n",
    "        \"\"\"\n",
    "        Traverses the decision tree to predict the class label for a given sample.\n",
    "\n",
    "        Parameters:\n",
    "        - sample: array-like of shape (n_features,)\n",
    "            The input sample.\n",
    "        - node: dict\n",
    "            The current node of the decision tree.\n",
    "\n",
    "        Returns:\n",
    "        label: int\n",
    "            The predicted class label.\n",
    "        \"\"\"\n",
    "\n",
    "        if 'label' in node:\n",
    "            return node['label']\n",
    "        else:\n",
    "            if sample[node['splitting_feature']] <= node['splitting_threshold']:\n",
    "                return self.__traverse_tree(sample, node['left_child'])\n",
    "            else:\n",
    "                return self.__traverse_tree(sample, node['right_child'])\n",
    "            \n",
    "    def __recursive_print(self, node, indent=\"\"):\n",
    "        \"\"\"Recursively print the decision tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node\n",
    "            The current node to be printed.\n",
    "        indent : str, default=\"\"\n",
    "            The indentation string for formatting the tree.\n",
    "        \"\"\"\n",
    "        if 'label' in node:\n",
    "            print(\"{}leaf - label: {} \".format(indent, node['label']))\n",
    "            return\n",
    "\n",
    "        print(\"{}id:{} - threshold: {}\".format(indent,\n",
    "              node['splitting_feature'], node['splitting_threshold']))\n",
    "\n",
    "        self.__recursive_print(node['left_child'], \"{}   \".format(indent))\n",
    "        self.__recursive_print(node['right_child'], \"{}   \".format(indent))\n",
    "\n",
    "    def print_tree(self):\n",
    "        \"\"\"Display the decision tree structure.\"\"\"\n",
    "        self.__recursive_print(self.tree)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Get parameters for this estimator.\n",
    "\n",
    "        Parameters:\n",
    "        - deep: bool, default=True\n",
    "            If True, will return the parameters for this estimator and contained subobjects that are estimators.\n",
    "\n",
    "        Returns:\n",
    "        params: mapping of string to any\n",
    "            Parameter names mapped to their values.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"random_state\": self.random_state,\n",
    "            \"max_depth\": self.max_depth,\n",
    "            \"min_sample_split\": self.min_sample_split,\n",
    "            \"criterion\": self.criterion,\n",
    "            \"min_info_gain\": self.min_info_gain,\n",
    "            \"max_features\": self.max_features,\n",
    "            \"max_thresholds\": self.max_thresholds,\n",
    "            \"class_weights\": self.class_weights,\n",
    "            \"verbose\": self.verbose\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set the parameters of this estimator.\n",
    "\n",
    "        Parameters:\n",
    "        - **params: dict\n",
    "            Estimator parameters.\n",
    "\n",
    "        Returns:\n",
    "        self: estimator instance\n",
    "            Estimator instance.\n",
    "        \"\"\"\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_breast_cancer(as_frame=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(data['data']), np.array(data['target']), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "dtc = DTC(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.fit(X_train, y_train)\n",
    "dtc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Dopo aver implementato l'albero, prima di poter andare a testarlo in maniera definitiva sui dati, vado ad effettuare un tuning degli iperparametri così da massimizzare le prestazioni del modello. Il tuning viene fatto utilizzando la libreria Hyperopt la quale va ad eseguire il train e calcolare lo score sul validation set con diverse combinazioni di iperparametri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials, SparkTrials\n",
    "\n",
    "train_pd = pd.read_parquet('./partial_results/df.train')\n",
    "val_pd = pd.read_parquet('./partial_results/df.val')\n",
    "\n",
    "X_train = np.array(train_pd.drop('is_laundering', axis=1))\n",
    "y_train = np.array(train_pd['is_laundering'])\n",
    "\n",
    "X_val = np.array(val_pd.drop('is_laundering', axis=1))\n",
    "y_val = np.array(val_pd['is_laundering'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il tuning degli iperparametri verrà fatto andando ad aggiungere il class_weighting. Questo perché in questo modo si creare bilanciamento nelle classi quando viene calcolata l'information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {0: 1, 1: 1/(np.sum(y_train)/len(y_train))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(params):\n",
    "        dtc = DTC(**params, random_state=0, class_weights=class_weights, verbose=True)\n",
    "        dtc.fit(X_train, y_train)\n",
    "        return -dtc.score(X_val, y_val)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli iperparametri vengono scelti in base ad intervalli di due valori ciascuno, così da ottenere più combinazioni. Nella scelta degli iperparametri non viene considerato il \"None\" quando si va a prendere la profondità massima, le max_features e i max_thresholds in quanto comporterebbe un enorme dispendio di tempo calcolare le diverse combinazioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choises = {\n",
    "    'max_depth': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20],\n",
    "    'min_sample_split': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20],\n",
    "    'criterion': ['gini', 'entropy', 'shannon'],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_thresholds': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
    "}\n",
    "\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', choises['max_depth']),\n",
    "    'min_sample_split': hp.choice('min_sample_split', choises['min_sample_split']),\n",
    "    'criterion': hp.choice('criterion', choises['criterion']),\n",
    "    'max_features': hp.choice('max_features', choises['max_features']),\n",
    "    'max_thresholds': hp.choice('max_thresholds', choises['max_thresholds'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn=objective_function, space=space, algo=tpe.suggest, max_evals=300, trials=trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta eseguito l'hyperparameter tuning, fatto per 600 iterazioni (1/10 delle possibili totali), i parametri scelti per il miglior score ottenuto, verranno utilizzati come parametri da inserire per inizializzare il decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in best.items():\n",
    "    print(f\"{key}: {choises[key][value]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test DTC\n",
    "\n",
    "In questa fase viene utilizzata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pd = pd.read_parquet('./partial_results/df.test')\n",
    "\n",
    "X_test = np.array(test_pd.drop('is_laundering', axis=1))\n",
    "y_test = np.array(test_pd['is_laundering'])\n",
    "\n",
    "cls_m = DTC(random_state=0, max_thresholds = 10, criterion='shannon', max_depth=25, min_sample_split=14, max_features='log2', verbose=True)\n",
    "cls_m.fit(X_res, y_res)\n",
    "cls_m.score(X_test, y_test)\n",
    "y_pred = cls_m.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calcola la matrice di confusione\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Etichette delle classi (sostituisci con i tuoi nomi di classi reali se necessario)\n",
    "class_names = ['0', '1']\n",
    "\n",
    "# Crea il grafico della matrice di confusione\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Matrice di Confusione')\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names, rotation=45)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# Aggiungi i valori delle celle nella matrice\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.xlabel('Classe Predetta')\n",
    "plt.ylabel('Classe Reale')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
